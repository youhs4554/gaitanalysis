{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from IPython.core.debugger import set_trace\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Output Format (BODY_25)\n",
    "<img src=\"https://github.com/CMU-Perceptual-Computing-Lab/openpose/raw/master/doc/media/keypoints_pose_25.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check cuda.is_available ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"cuda_available : {}, device : {}\".format(cuda_available, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_pairs = [\n",
    "              #(1,8),\n",
    "              #(1,2),\n",
    "              #(1,5),\n",
    "              #(2,3),\n",
    "              #(3,4),\n",
    "              #(5,6),\n",
    "              #(6,7),\n",
    "              (8,9),\n",
    "              (9,10),\n",
    "              (10,11),\n",
    "              (8,12),\n",
    "              (12,13),\n",
    "              (13,14),\n",
    "              #(1,0),\n",
    "              #(0,15),\n",
    "              #(15,17),\n",
    "              #(0,16),\n",
    "              #(16,18),\n",
    "              #(2,17),\n",
    "              #(5,18),\n",
    "              (14,19),\n",
    "              (19,20),\n",
    "              (14,21),\n",
    "              (11,22),\n",
    "              (22,23),\n",
    "              (11,24)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 300\n",
    "FPS = 24.0\n",
    "\n",
    "keypoints_frame = pd.read_pickle(\"../../preprocess/data/keypoints_dataframe.pkl\")\n",
    "\n",
    "def drop_huge_seq(keypoints_frame, save_path=\"../../preprocess/data/keypoints_dataframe_drop.pkl\"):\n",
    "    if os.path.exists(save_path):\n",
    "        print('Already dropped! Return...')\n",
    "        return\n",
    "    \n",
    "    vids = list(set(keypoints_frame.vid))\n",
    "\n",
    "    for i in tqdm(range(len(vids)), desc='DropInputSeq '):\n",
    "        slice_df = keypoints_frame.loc[keypoints_frame.vid==vids[i]].iloc[:, 2:]\n",
    "        if slice_df.values.shape[0] > MAXLEN:\n",
    "            keypoints_frame.iloc[slice_df.index, 2:] = np.nan * np.empty_like(slice_df.values)\n",
    "\n",
    "    # drop Nans !\n",
    "    keypoints_frame = keypoints_frame.dropna()\n",
    "    keypoints_frame.to_pickle(\"../../preprocess/data/keypoints_dataframe_drop.pkl\")\n",
    "\n",
    "    \n",
    "# drop huge seq\n",
    "drop_huge_seq(keypoints_frame, save_path=\"../../preprocess/data/keypoints_dataframe_drop.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pid2vid(pid):\n",
    "    num, test_id, trial_id = pid.split('_')\n",
    "    return '_'.join([num, 'test', test_id, 'trial', trial_id]) + '.avi'\n",
    "    \n",
    "\n",
    "def vid2pid(vid):\n",
    "    split = os.path.splitext(vid)[0].split('_')\n",
    "    return '_'.join([split[0], split[2], split[4]])\n",
    "        \n",
    "    \n",
    "class GAITDataset(Dataset):\n",
    "    def __init__(self, keypoints_pkl_file, targets_pkl_file, phase, split_ratio, maxlen=300, fps=24.0, r_seed=3):\n",
    "        keypoints_frame = pd.read_pickle(keypoints_pkl_file)\n",
    "        targets_frame = pd.read_pickle(targets_pkl_file)[['Toe In / Out/L','start','end']]\n",
    "        self.maxlen = maxlen\n",
    "        self.fps = fps\n",
    "                        \n",
    "        vids = list(set(keypoints_frame.vid))\n",
    "\n",
    "        # reindex tgt data (to filter-out valid vids)\n",
    "        pids = []\n",
    "        for vid in vids:\n",
    "            pids.append(vid2pid(vid))\n",
    "\n",
    "        targets_frame = targets_frame.reindex(pids, fill_value=0.0)\n",
    "        \n",
    "        # vids without zero values in target df\n",
    "        vids = [ pid2vid(pid) for pid in targets_frame.index.values ]\n",
    "\n",
    "        # target dataframe (minmax scaled)\n",
    "        #self.target_scaler = MinMaxScaler(feature_range=(0, 1.0))\n",
    "        self.target_scaler = StandardScaler()\n",
    "        scaled_values = self.target_scaler.fit_transform(targets_frame.values[:,:-2])\n",
    "        targets_frame.loc[:,:-2] = scaled_values\n",
    "        \n",
    "        self.keypoints_frame = keypoints_frame\n",
    "        self.targets_frame = targets_frame\n",
    "        \n",
    "        import random\n",
    "        random.seed(r_seed)\n",
    "        random.shuffle(vids)  # shuffle vids inplace, before datasplit\n",
    "        \n",
    "        if phase=='train':\n",
    "            self.vids = vids[:int(len(vids)*split_ratio)]\n",
    "        elif phase=='test':\n",
    "            self.vids = vids[-int(len(vids)*split_ratio):]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        cur_keypoints_frame = self.keypoints_frame.loc[self.keypoints_frame.vid==self.vids[idx]]\n",
    "        \n",
    "        pid, _, test_ix, _, trial_ix, *_ = os.path.splitext(self.vids[idx])[0].split('_')\n",
    "        \n",
    "        target_id = '_'.join([pid, test_ix, trial_ix])\n",
    "        target_data = self.targets_frame.loc[target_id].values\n",
    "        \n",
    "        # parse target data\n",
    "        targets, (start, end) = target_data[:-2], target_data[-2:]\n",
    "        \n",
    "        # convert time -> frame_ix\n",
    "        start_ix = int(start * FPS)\n",
    "        end_ix = int(end*FPS)\n",
    "\n",
    "        # select keypoints based on start/end time\n",
    "        keypoints_seq = cur_keypoints_frame.query(f'{start_ix} < frame_cnt < {end_ix}').iloc[:, 2:].values\n",
    "        \n",
    "        # seq_len before padding\n",
    "        seq_len = len(keypoints_seq)\n",
    "        \n",
    "        # zero padding\n",
    "        keypoints_seq = np.pad(keypoints_seq, ((0,self.maxlen-len(keypoints_seq)),(0,0)),\n",
    "                                               'constant', constant_values=0).transpose(1,0)\n",
    "        \n",
    "        \n",
    "        sample = {'keypoints_seq': torch.tensor(keypoints_seq, dtype=torch.float32),\n",
    "                  'targets': torch.tensor(targets, dtype=torch.float32),\n",
    "                  'seq_len': torch.tensor(seq_len, dtype=torch.int32)}\n",
    "        \n",
    "        return sample\n",
    "\n",
    "# dataset path\n",
    "keypoints_pkl_file = \"../../preprocess/data/keypoints_dataframe_drop.pkl\"\n",
    "targets_pkl_file = \"../../preprocess/data/targets_dataframe.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = { phase : GAITDataset(keypoints_pkl_file, targets_pkl_file, phase=phase, split_ratio=split_ratio, \n",
    "                                maxlen=MAXLEN, fps=FPS) \\\n",
    "                for phase,split_ratio in zip(['train', 'test'], [0.8, 0.2]) }\n",
    "\n",
    "dataloader = { phase : DataLoader(dataset[phase],\n",
    "                        batch_size=8*torch.cuda.device_count(),\n",
    "                        shuffle=True,\n",
    "                        num_workers=16) \\\n",
    "                    for phase in ['train', 'test'] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for phase in ['train', 'test']:\n",
    "    df_list.append(\n",
    "        dataset[phase].targets_frame\n",
    "    )\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "\n",
    "\n",
    "single_columns = df.columns[:3]\n",
    "pair_columns = df.columns[3:-2]\n",
    "\n",
    "def Normal(x, mu=0.0, sigma=1.0):\n",
    "    return (1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (x - mu)**2 / (2 * sigma**2) ))\n",
    "\n",
    "\n",
    "def visualize_dist_of_factors(df, columns, fig_size=(7,7), subplot_coord=(3,1), paired=False, title=None):\n",
    "\n",
    "    sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, axes = plt.subplots(*subplot_coord, figsize=fig_size, sharex=False)\n",
    "    if title:\n",
    "        f.suptitle(title, fontsize=16, y=2.0)\n",
    "    sns.despine(left=True)\n",
    "    \n",
    "    for i in range(len(columns)):\n",
    "        ax = axes[i//2, i%2] if paired else axes[i]\n",
    "        values = df[columns[i]].values\n",
    "        mu, sigma = values.mean(), values.std()\n",
    "        cnt, bins, _ = ax.hist(values, 60, density=True)\n",
    "        \n",
    "        ax.plot(bins, Normal(bins, mu, sigma))\n",
    "        ax.set_title(columns[i])\n",
    "        plt.tight_layout()\n",
    "    plt.setp(axes, yticks=[])\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "visualize_dist_of_factors(df, single_columns, fig_size=(7,7), subplot_coord=(3,1), paired=False, title='Single Factor Distributions')\n",
    "\n",
    "visualize_dist_of_factors(df, pair_columns, fig_size=(9,9), subplot_coord=(7,2), paired=True, title='Paired Factor Distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d_with_same_padding(nn.Conv1d):\n",
    "    def __init__(self, in_channels,\n",
    "                       out_channels,\n",
    "                       kernel_size,\n",
    "                       stride=1,\n",
    "                       padding=0,\n",
    "                       dilation=1,\n",
    "                       groups=1,\n",
    "                       bias=True,\n",
    "                       padding_type='same'):\n",
    "        \n",
    "        super(Conv1d_with_same_padding, self).__init__(in_channels,\n",
    "                                     out_channels,\n",
    "                                     kernel_size,\n",
    "                                     stride,\n",
    "                                     padding,\n",
    "                                     dilation,\n",
    "                                     groups,\n",
    "                                     bias)\n",
    "        \n",
    "        self.padding_type = padding_type\n",
    "    \n",
    "    def forward(self, x, debug=False):\n",
    "        _, _, input_length = x.size()\n",
    "        if debug:\n",
    "            set_trace()\n",
    "        if self.padding_type == 'same':\n",
    "            padding_need = round((input_length * (self.stride[0]-1) + self.kernel_size[0] - self.stride[0]) / 2)\n",
    "            \n",
    "        return F.conv1d(x, self.weight, self.bias, self.stride, \n",
    "                        padding_need, self.dilation, self.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, C_in, C_out, pool, highway=True):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.pool = pool\n",
    "        self.highway = highway\n",
    "                \n",
    "        stride = 1\n",
    "        \n",
    "        if C_in != C_out:\n",
    "            C = C_out\n",
    "        else:\n",
    "            C = C_in = C_out\n",
    "            \n",
    "        if pool:\n",
    "            # input dimension matchig\n",
    "            self.conv_1x1_matching = Conv1d_with_same_padding(C_in, C, kernel_size=1, stride=1, padding_type='same')\n",
    "            self.bn_1x1_matching = nn.BatchNorm1d(C)\n",
    "\n",
    "            # for pooling of residual path\n",
    "            stride = 2\n",
    "            self.conv_1x1_pool = Conv1d_with_same_padding(C_in, C, kernel_size=1, stride=2, padding_type='same')\n",
    "            self.bn_1x1_pool= nn.BatchNorm1d(C)\n",
    "                \n",
    "        # conv_1x1_a : reduce number of channels by factor of 4 (output_channel = C/4)\n",
    "        self.conv_1x1_a = Conv1d_with_same_padding(C, int(C/4), kernel_size=1, stride=stride, padding_type='same')\n",
    "        self.bn_1x1_a = nn.BatchNorm1d(int(C/4))\n",
    "        \n",
    "        # conv_3x3_b : more wide receptive field (output_channel = C/4)\n",
    "        self.conv_3x3_b = Conv1d_with_same_padding(int(C/4), int(C/4), kernel_size=3, stride=1, padding_type='same')\n",
    "        self.bn_3x3_b = nn.BatchNorm1d(int(C/4))\n",
    "        \n",
    "        # conv_1x1_c : recover org channel C (output_channel = C)\n",
    "        self.conv_1x1_c = Conv1d_with_same_padding(int(C/4), C, kernel_size=1, stride=1, padding_type='same')\n",
    "        self.bn_1x1_c = nn.BatchNorm1d(C)\n",
    "        \n",
    "        if highway:\n",
    "            # conv_1x1_g : gating for highway network\n",
    "            self.conv_1x1_g = Conv1d_with_same_padding(C, C, kernel_size=1, stride=1, padding_type='same')\n",
    "        \n",
    "        # output\n",
    "        self.bn_1x1_out = nn.BatchNorm1d(C)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            x : size = (batch, C, maxlen)\n",
    "        '''\n",
    "        \n",
    "        res = x\n",
    "        \n",
    "        if self.pool:\n",
    "            # input dimension matching with 1x1 conv\n",
    "            x = self.conv_1x1_matching(x)\n",
    "            x = self.bn_1x1_matching(x)\n",
    "            \n",
    "            # pooling of residual path\n",
    "            res = self.conv_1x1_pool(res)\n",
    "            res = self.bn_1x1_pool(res)\n",
    "        \n",
    "        # 1x1_a (C/4)\n",
    "        x = self.conv_1x1_a(x)\n",
    "        x = self.bn_1x1_a(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # 3x3_b (C/4)\n",
    "        x = self.conv_3x3_b(x)\n",
    "        x = self.bn_3x3_b(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # 1x1_c (C)\n",
    "        x = self.conv_1x1_c(x)\n",
    "        x = self.bn_1x1_c(x)\n",
    "        \n",
    "        if self.highway:\n",
    "            # gating mechanism from \"highway network\"\n",
    "            \n",
    "            # gating factors controll intensity between x and f(x)\n",
    "            # gating = 1.0 (short circuit) --> output is identity (same as initial input)\n",
    "            # gating = 0.0 (open circuit)--> output is f(x) (case of non-residual network)\n",
    "            gating = torch.sigmoid(self.conv_1x1_g(x))\n",
    "            \n",
    "            # apply gating mechanism\n",
    "            x = gating * res + (1.0 - gating) * F.relu(x)\n",
    "\n",
    "            \n",
    "        else:\n",
    "            # normal residual ops (addition)\n",
    "            x = F.relu(x) + res\n",
    "            \n",
    "#         x = self.bn_1x1_out(x)\n",
    "#         x = F.relu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get statiscal values (mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []  # list of df to merge\n",
    "\n",
    "for phase in ['train', 'test']:\n",
    "    df_list.append(\n",
    "        dataset[phase].targets_frame.iloc[:,:-2]\n",
    "    )\n",
    "\n",
    "df = pd.concat(df_list)  #  concat\n",
    "\n",
    "# mean and std from data\n",
    "mu = df.values.mean(0)\n",
    "sigma = df.values.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self, *shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)\n",
    "\n",
    "class GAP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAP, self).__init__()\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            x : size = (B, C, L)\n",
    "        '''\n",
    "        return torch.mean(x, 2)\n",
    "        \n",
    "        \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, target_size, num_layers = [3,4,6], num_filters = [64,128,128]):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        def res_blocks(residual_blocks, num_layers, num_filters, block_ix, pool_first_layer=True):\n",
    "            block_layers = num_layers[block_ix]\n",
    "\n",
    "            for i in range(block_layers):\n",
    "                # default values\n",
    "                pool = False\n",
    "                block_filters = num_filters[block_ix]\n",
    "                \n",
    "                C_in = C_out = block_filters\n",
    "                \n",
    "                if pool_first_layer and i==0:\n",
    "                    pool = True\n",
    "                if i==0 and block_ix > 0:\n",
    "                    C_in = num_filters[block_ix-1]\n",
    "                    \n",
    "                print(f\"layer : {i}, block : {block_ix}, C_in/C_out : {C_in}/{C_out}\")\n",
    "                residual_blocks.append(ResidualBlock(C_in=C_in, C_out=C_out,pool=pool, highway=True))\n",
    "                \n",
    "        residual_blocks = []\n",
    "\n",
    "        for i in range(len(num_layers)):\n",
    "            pool_first_layer = True\n",
    "            if i == 0:\n",
    "                pool_first_layer = False\n",
    "            res_blocks(residual_blocks, num_layers=num_layers, num_filters=num_filters, block_ix=i,\n",
    "                       pool_first_layer=pool_first_layer)\n",
    "\n",
    "        self.model = nn.Sequential(nn.Conv1d(input_size, num_filters[0], kernel_size=4, stride=2, padding=1),\n",
    "                                   nn.BatchNorm1d(num_filters[0]),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(kernel_size=(3,), stride=2,),\n",
    "                                   nn.Conv1d(num_filters[0], num_filters[0], kernel_size=4, stride=2, padding=1),\n",
    "                                   nn.BatchNorm1d(num_filters[0]),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(kernel_size=(3,), stride=2,),\n",
    "                                   nn.Conv1d(num_filters[0], num_filters[0], kernel_size=4, stride=2, padding=1),\n",
    "                                   nn.BatchNorm1d(num_filters[0]),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool1d(kernel_size=(3,), stride=2,),\n",
    "                                   #*residual_blocks,\n",
    "                                   GAP(),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(num_filters[0], target_size),\n",
    "                                   )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            x : size = (batch, input_size, maxlen)\n",
    "        '''\n",
    "        return self.model(x)        \n",
    "        \n",
    "        \n",
    "\n",
    "net = Net(input_size=39,\n",
    "          target_size=len(dataset['train'].targets_frame.columns)-2,\n",
    "          num_layers = [1,1,1], num_filters = [128,128,128])\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    net = nn.DataParallel(net)\n",
    "else:\n",
    "    print(\"Single GPU mode\")\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define criterion\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.L1Loss()\n",
    "\n",
    "def Normal(x, mu=0.0, sigma=1.0):\n",
    "    if type(x)==torch.Tensor:\n",
    "        x = x.detach().cpu().numpy()\n",
    "        \n",
    "    res = (1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (x - mu)**2 / (2 * sigma**2) )).astype(np.float32)\n",
    "    return torch.tensor(res).float().to(device)\n",
    "\n",
    "\n",
    "df_list = []  # list of df to merge\n",
    "\n",
    "for phase in ['train', 'test']:\n",
    "    df_list.append(\n",
    "        dataset[phase].targets_frame.iloc[:,:-2]\n",
    "    )\n",
    "\n",
    "df = pd.concat(df_list)  #  concat\n",
    "\n",
    "# mean and std from data\n",
    "mu = df.values.mean(0)\n",
    "sigma = df.values.std(0)\n",
    "\n",
    "# criterion = lambda x, y : torch.mean( torch.pow( (x - y), 2) )\n",
    "# criterion = lambda x, y : torch.mean( torch.pow(1-Normal(x, mu, sigma), 2) * torch.pow( (x - y), 2) )\n",
    "# criterion = lambda x, y : torch.mean( torch.abs(torch.log( 1. / (Normal(x, mu, sigma)+1e-7) )) * torch.pow( (x - y), 2) )\n",
    "# criterion = lambda x, y : torch.mean( \n",
    "#     torch.tanh( torch.abs(torch.log( 1. / (Normal(x, mu, sigma)) )) ) * torch.pow( (x - y), 2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cost history saving..\n",
    "history = {'train': [],\n",
    "           'test': []}\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "# optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "# optimizer = optim.RMSprop(net.parameters(), lr=1e-2)\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-2, momentum=0.9, nesterov=True)\n",
    "\n",
    "epoch_loss = {'train': 0.0, 'test': 0.0}\n",
    "pred_and_gt = { k:[] for k in df.columns.values }\n",
    "\n",
    "end_epoch = 20\n",
    "\n",
    "print(\"Start training loop...\")\n",
    "\n",
    "for epoch in range(1,end_epoch+1):        \n",
    "    for phase in ['train', 'test']:\n",
    "        if phase=='train':\n",
    "            net.train()\n",
    "        elif phase=='test':\n",
    "            net.eval()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "\n",
    "        for idx, batch_item in enumerate(dataloader[phase]):\n",
    "            input, target, seq_len = batch_item['keypoints_seq'].to(device), batch_item['targets'].to(device), \\\n",
    "                                      batch_item['seq_len'].to(device)\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.set_grad_enabled(phase=='train'):\n",
    "                # feed data to network\n",
    "                res = net(input)\n",
    "                \n",
    "                # compute loss\n",
    "                loss = criterion(res, target)\n",
    "                print(loss.item())\n",
    "                if phase=='train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                                    \n",
    "                if epoch==end_epoch and phase=='test':\n",
    "                    for i,col in enumerate(df.columns.values):\n",
    "                        pred_and_gt[col].append([res[:,i].detach().cpu().numpy(), target[:,i].cpu().numpy()])\n",
    "                        \n",
    "            running_loss += loss.item() * len(input)\n",
    "            \n",
    "            \n",
    "        avg_loss = running_loss / len(dataloader[phase].dataset)\n",
    "                \n",
    "        epoch_loss[phase] += avg_loss\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print('=================={}========================'.format(phase.upper()))\n",
    "            print('EPOCH : {}, WEIGHTED_SQUARED_ERROR (WSE) : {:.4f}'.format(epoch, epoch_loss[phase] / 5))\n",
    "            history[phase].append(epoch_loss[phase] / 5)\n",
    "            \n",
    "            # init epoch_loss at its own phase\n",
    "            epoch_loss[phase] = 0.0\n",
    "\n",
    "print(\"Done!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot learning curve\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.arange(len(history['train'])*10, step=10), history['train'], label='train', color='b')\n",
    "ax.plot(np.arange(len(history['test'])*10, step=10), history['test'], label='test', color='orange')\n",
    "\n",
    "ax.set_title('Learning Curve')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "print(\"Saving learning curve...\")\n",
    "fig.savefig('learning.png', dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "scaler = dataset['train'].target_scaler\n",
    "data = collections.defaultdict(list)\n",
    "\n",
    "pp = []\n",
    "gg = []\n",
    "for i,col in enumerate(pred_and_gt.keys()):\n",
    "    transposed_data = list(zip(*pred_and_gt[col]))\n",
    "    preds = scaler.inverse_transform(np.concatenate(transposed_data[0]))\n",
    "    gts = scaler.inverse_transform(np.concatenate(transposed_data[1]))\n",
    "    \n",
    "    pp.append(preds)\n",
    "    gg.append(gts)\n",
    "    \n",
    "    for p,g in zip(preds, gts):\n",
    "        data[\"name\"].append(col)\n",
    "        data[\"pred\"].append(p)\n",
    "        data[\"gt\"].append(g)\n",
    "        \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "grid = sns.FacetGrid(df, col=\"name\", col_wrap=3, height=5, aspect=1.5, sharex=False, sharey=False)\n",
    "grid.map(plt.scatter, \"pred\", \"gt\", color='g', label='data')\n",
    "\n",
    "for i,(preds,gts) in enumerate(zip(pp,gg)):\n",
    "    grid.axes[i].plot([min(preds), max(preds)], [min(gts), max(gts)], 'r--', label='GT=PRED')\n",
    "    grid.axes[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
