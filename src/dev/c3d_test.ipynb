{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "from itertools import chain\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from IPython.core.debugger import set_trace\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from natsort import natsorted\n",
    "import collections\n",
    "from IPython import display\n",
    "import pylab as pl\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.helper import predefined_split\n",
    "from skorch import callbacks\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import defaultdict         \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics.regression import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.model_selection import KFold\n",
    "import c3d_wrapper\n",
    "from data_utils import *\n",
    "from models import *\n",
    "from params import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks for skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Callback\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def to_tensor_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.permute(0,3,1,2)\n",
    "    x = x.view(x.size(0), 1, 64, 64)\n",
    "    return x\n",
    "\n",
    "class Save_Reconstruction_Results(Callback):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        for name in ['train', 'valid']:\n",
    "            dataset = kwargs['dataset_'+name]\n",
    "            rand_ix = np.random.randint(len(dataset))\n",
    "            X,y = dataset[rand_ix]\n",
    "            \n",
    "            save_dir = os.path.join(self.path, name)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            \n",
    "            # target img\n",
    "            y = y.numpy().transpose(1,2,3,0)  # (maxlen,h,w,3)\n",
    "            \n",
    "            # predicted img\n",
    "            pred = net.predict(X[None,:])[0].transpose(1,2,3,0) # (maxlen,h,w,3)\n",
    "            \n",
    "            for sub_name,pic in zip(['target', 'pred'], [y,pred]):\n",
    "                pic = to_tensor_img(torch.from_numpy(pic))\n",
    "                save_image(pic, os.path.join(save_dir,sub_name+'.png'))\n",
    "\n",
    "                \n",
    "def fetch_samples_from_dataset(dataset):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for item in dataset:\n",
    "        X.append(item[0].numpy())\n",
    "        Y.append(item[1].numpy())\n",
    "        \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "class Report_Regression_Results(Callback):\n",
    "    def __init__(self, columns, scaler=None, period=5):\n",
    "        self.columns = columns\n",
    "        self.scaler = scaler\n",
    "        self.period = period\n",
    "        \n",
    "        self.loss_history = defaultdict(list)\n",
    "    \n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        if len(net.history) % self.period == 0:\n",
    "            for phase in ['train', 'valid']:\n",
    "                self.loss_history[phase].append(net.history[-1, f'{phase}_loss'])\n",
    "            \n",
    "            for phase in ['train', 'valid']:\n",
    "                dataset = kwargs['dataset_'+phase]\n",
    "\n",
    "                X, y_true = fetch_samples_from_dataset(dataset)\n",
    "                y_pred = net.predict(X)\n",
    "\n",
    "                report_lerning_process(columns=self.columns,\n",
    "                                       epoch=len(net.history),\n",
    "                                       phase=phase,\n",
    "                                       y_pred=self.scaler.inverse_transform(y_pred) if self.scaler else y_pred,\n",
    "                                       y_true=self.scaler.inverse_transform(y_true) if self.scaler else y_true,\n",
    "                                       loss_history=self.loss_history)\n",
    "        \n",
    "    \n",
    "    def score(y_pred, y_true):\n",
    "        print('MAE :', mean_absolute_error(y_true, y_pred, multioutput='raw_values'))\n",
    "        print('MSE :', mean_squared_error(y_true, y_pred, multioutput='raw_values'))\n",
    "        print('RMSE :', np.sqrt(mean_squared_error(y_true, y_pred, multioutput='raw_values')))\n",
    "        print('R^2 : ', r2_score(y_true, y_pred, multioutput='variance_weighted'))\n",
    "        print('Explained variation : ',explained_variance_score(y_true, y_pred, multioutput='variance_weighted'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "class MyCriterion(_Loss):\n",
    "    def __init__(self):\n",
    "        super(MyCriterion, self).__init__()\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        valid_mask = ~(y.view(y.size(0),FRAME_MAXLEN,-1)==0).all(dim=2)\n",
    "        valid_mask = valid_mask.float()\n",
    "        return torch.mean(torch.sum((valid_mask * ((x-y)**2).mean((1,3,4))),1)/torch.sum(valid_mask,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hossay/anaconda3/envs/torch/lib/python3.6/site-packages/distributed/bokeh/core.py:74: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_loop(model,\n",
    "                          super_class=NeuralNetRegressor,\n",
    "                          task = 'regression@pretrained',\n",
    "                          n_splits=5,\n",
    "                          feature_extraction_model=None, feature_layer='conv1',\n",
    "                          # dataset path\n",
    "                          input_file = \"../../preprocess/data/person_detection_and_tracking_results_drop.pkl\",\n",
    "                          target_file = \"../../preprocess/data/targets_dataframe.pkl\",\n",
    "                          callback_list=['early_stop', 'prog_bar', 'report_regression_results'],\n",
    "                          scores=['MAPE', 'MAE', 'RMSE', 'R2', 'Explained variation']):\n",
    "    \n",
    "\n",
    "    data_dict = prepare_dataset(input_file, target_file,\n",
    "                                feature_extraction_model=feature_extraction_model, layer=feature_layer)\n",
    "\n",
    "    # holdouf test set for final evaluation\n",
    "    test_dataset = dataset_init(task, \n",
    "                                data_dict['test_X'], data_dict['test_y'],\n",
    "                                scaler=data_dict['scaler'], name='test')\n",
    "    \n",
    "    # can be modified !\n",
    "    scores = { k:[] for k in scores }\n",
    "    \n",
    "    data_locations = np.array(data_dict['train_vids'])\n",
    "\n",
    "    input_df = data_dict['input_df']\n",
    "    target_df = data_dict['target_df']\n",
    "    scaler = data_dict['scaler']\n",
    "    \n",
    "    if callback_list:\n",
    "        call_backs_map = {'early_stop': callbacks.EarlyStopping(),\n",
    "                          'prog_bar': callbacks.ProgressBar(),\n",
    "                          'lr_scheduler': callbacks.LRScheduler(policy='WarmRestartLR'),\n",
    "                          'report_regression_results': Report_Regression_Results(columns=target_columns, period=1, scaler=scaler),\n",
    "                          'save_reconstruction_results': Save_Reconstruction_Results(path='./results')\n",
    "                         }\n",
    "\n",
    "        callback_list = [ call_backs_map.get(cb) for cb in callback_list ]\n",
    "    \n",
    "    # K-fold CV\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    \n",
    "    for train, valid in kf.split(data_locations):\n",
    "        \n",
    "        # split trainset with train/valid\n",
    "        train_split, valid_split = data_locations[train], data_locations[valid]\n",
    "\n",
    "        train_X, train_y = filter_input_df_with_vids(input_df,train_split), filter_target_df_with_vids(target_df,train_split)\n",
    "        valid_X, valid_y = filter_input_df_with_vids(input_df,valid_split), filter_target_df_with_vids(target_df,valid_split)\n",
    "\n",
    "        # dsataset !!\n",
    "        train_dataset = dataset_init(task, train_X, train_y, scaler=scaler, name='train')\n",
    "        valid_dataset = dataset_init(task, valid_X, valid_y, scaler=scaler, name='valid')\n",
    "                \n",
    "        # init net\n",
    "        net = super_class(\n",
    "            model,\n",
    "            batch_size=4,\n",
    "            max_epochs=50,\n",
    "            lr=1e-4,\n",
    "            optimizer=torch.optim.SGD,\n",
    "            optimizer__weight_decay=1.0,\n",
    "            device='cuda',\n",
    "            #criterion=torch.nn.modules.loss.SmoothL1Loss,\n",
    "            train_split=predefined_split(valid_dataset),\n",
    "            # Shuffle training data on each epoch\n",
    "            iterator_train__shuffle=True,\n",
    "            warm_start=False, # re-init the module\n",
    "            callbacks=callback_list,\n",
    "        )\n",
    "        \n",
    "        with joblib.parallel_backend('dask', scheduler_host='127.0.0.1:8786'):\n",
    "            # implicit train/validate loop for each CV split\n",
    "            net.fit(train_dataset, y=None)\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize with pretrained weight file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# pretrained c3d net( tensorflow )\n",
    "tf_model = TF_Model()\n",
    "\n",
    "visualize_conv_featsmap('7157030_test_0_trial_1', tf_model, layer='conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=492), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.0301\u001b[0m        \u001b[32m1.0535\u001b[0m  5.4426\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=492), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m1.0041\u001b[0m        \u001b[32m1.0306\u001b[0m  4.7663\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=492), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3        \u001b[36m0.9824\u001b[0m        \u001b[32m1.0187\u001b[0m  4.5160\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=492), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4        \u001b[36m0.9751\u001b[0m        \u001b[32m1.0112\u001b[0m  5.0705\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=492), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.9678\u001b[0m        \u001b[32m1.0043\u001b[0m  3.9598\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=492), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.9565\u001b[0m        \u001b[32m1.0018\u001b[0m  3.6643\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=492), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m0.9476\u001b[0m        \u001b[32m0.9975\u001b[0m  4.7994\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=492), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8        \u001b[36m0.9424\u001b[0m        \u001b[32m0.9934\u001b[0m  4.0560\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=492), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9        \u001b[36m0.9339\u001b[0m        \u001b[32m0.9891\u001b[0m  4.4385\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=492), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10        0.9405        \u001b[32m0.9862\u001b[0m  3.7133\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=492), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hossay/anaconda3/envs/torch/lib/python3.6/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11        \u001b[36m0.9295\u001b[0m        \u001b[32m0.9842\u001b[0m  4.0691\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=492), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "task = 'regression@pretrained'\n",
    "# task = 'reconstruction@pretrained'\n",
    "# task = 'reconstruction@fromscratch'\n",
    "\n",
    "super_class = NeuralNetRegressor\n",
    "if task.split('@')[1]=='fromscratch':\n",
    "    super_class = AutoEncoderNet\n",
    "\n",
    "cross_validation_loop(super_class=super_class,\n",
    "                      #feature_extraction_model=tf_model, feature_layer='conv5',\n",
    "                      model=eval('_'.join(task.split('@')).capitalize()),\n",
    "                      task=task,\n",
    "                      callback_list=['prog_bar', 'report_regression_results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
