{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "\n",
    "from itertools import chain\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from IPython.core.debugger import set_trace\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from natsort import natsorted\n",
    "import collections\n",
    "from IPython import display\n",
    "import pylab as pl\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.helper import predefined_split\n",
    "from skorch import callbacks\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import defaultdict         \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics.regression import median_absolute_error, mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.model_selection import KFold\n",
    "import c3d_wrapper\n",
    "from data_utils import *\n",
    "import models\n",
    "from params import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF model for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained c3d net( tensorflow )\n",
    "# tf_model = TF_Model()\n",
    "\n",
    "#visualize_conv_featsmap('7157030_test_0_trial_1', tf_model, layer='conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Callback\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def to_tensor_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.permute(0,3,1,2)\n",
    "    x = x.view(x.size(0), 1, 64, 64)\n",
    "    return x\n",
    "\n",
    "class Save_Reconstruction_Results(Callback):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        for name in ['train', 'valid']:\n",
    "            dataset = kwargs['dataset_'+name]\n",
    "            rand_ix = np.random.randint(len(dataset))\n",
    "            X,y = dataset[rand_ix]\n",
    "            \n",
    "            save_dir = os.path.join(self.path, name)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            \n",
    "            # target img\n",
    "            y = y.numpy().transpose(1,2,3,0)  # (maxlen,h,w,3)\n",
    "            \n",
    "            # predicted img\n",
    "            pred = net.predict(X[None,:])[0].transpose(1,2,3,0) # (maxlen,h,w,3)\n",
    "            \n",
    "            for sub_name,pic in zip(['target', 'pred'], [y,pred]):\n",
    "                pic = to_tensor_img(torch.from_numpy(pic))\n",
    "                save_image(pic, os.path.join(save_dir,sub_name+'.png'))\n",
    "\n",
    "                \n",
    "def fetch_samples_from_dataset(dataset):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for item in dataset:\n",
    "        X.append(item[0].numpy())\n",
    "        Y.append(item[1].numpy())\n",
    "        \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def check_array(arr):\n",
    "    dim = arr.shape[1]\n",
    "    \n",
    "    # select non-nan rows\n",
    "    try:\n",
    "        arr = arr[np.where(~np.isnan(arr))].reshape(-1, dim)\n",
    "    except:\n",
    "        set_trace()\n",
    "        \n",
    "    # select non-inf rows\n",
    "    arr = arr[(arr!=np.inf).all(1)]\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def mape(y_true, y_pred, scaler=None, reduce_all=True):\n",
    "    if scaler:\n",
    "        # when skorch callbacks is executed for epoch_scoring\n",
    "        y_pred = scaler.inverse_transform(y_pred)\n",
    "        y_true = scaler.inverse_transform(y_true)\n",
    "        \n",
    "    with np.errstate(divide='ignore'):\n",
    "        mape = np.abs((y_true-y_pred)/y_true)\n",
    "\n",
    "    if reduce_all:\n",
    "        return check_array(mape).mean()\n",
    "    else:\n",
    "        return check_array(mape).mean(0)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred, reduce_all=True):\n",
    "    if reduce_all:\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    else:\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred, multioutput='raw_values'))\n",
    "\n",
    "def record_score(y_pred, y_true, epoch, phase, log_path=\"score_log.txt\"):\n",
    "    MAE = mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
    "    MAPE = mape(y_true, y_pred, reduce_all=False)\n",
    "    RMSE = root_mean_squared_error(y_true, y_pred, reduce_all=False)\n",
    "    R2 = r2_score(y_true, y_pred, multioutput='raw_values')\n",
    "    EV = explained_variance_score(y_true, y_pred, multioutput='raw_values')\n",
    "    \n",
    "    deco_ = f\"=== EPOCH-{epoch} / {phase} === summary\"\n",
    "    mae_ = f\"MAE : {MAE}\"\n",
    "    mape_ = f\"MAPE : {MAPE}\"\n",
    "    rmse_ = f\"RMSE : {RMSE}\"\n",
    "    r2_ = f\"R^2 : {R2}\"\n",
    "    ev_ = f\"Explained variation : {EV}\"\n",
    "    \n",
    "    msg = '\\n'.join([deco_, mae_, mape_, rmse_, r2_, ev_])\n",
    "    os.system(\"echo \\'{}\\' >> {}\".format(msg, log_path))\n",
    "    \n",
    "    print(f'resulting file is saved to \\\"{log_path}\\\"')\n",
    "\n",
    "class Report_Regression_Results(Callback):\n",
    "    def __init__(self, columns, scores, scaler=None, period=5, log_path=\"score_log.txt\"):\n",
    "        self.columns = columns\n",
    "        self.scores = scores\n",
    "        self.scaler = scaler\n",
    "        self.period = period\n",
    "        \n",
    "        self.loss_history = defaultdict(list)\n",
    "        if os.path.exists(log_path):\n",
    "            os.system(f\"rm {log_path}\")\n",
    "            \n",
    "        self.log_path = log_path\n",
    "        self.epoch_scores = { k:[] for k in scores }\n",
    "        \n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        if len(net.history) % self.period == 0:\n",
    "            for phase in ['train', 'valid']:\n",
    "                self.loss_history[phase].append(net.history[-1, f'{phase}_loss'])\n",
    "            \n",
    "            for phase in ['train', 'valid']:\n",
    "                dataset = kwargs['dataset_'+phase]\n",
    "\n",
    "                X, y_true = fetch_samples_from_dataset(dataset)\n",
    "                y_pred = net.predict(X)\n",
    "                if self.scaler:\n",
    "                    y_pred = self.scaler.inverse_transform(y_pred)\n",
    "                    y_true = self.scaler.inverse_transform(y_true)\n",
    "                \n",
    "                record_score(y_pred, y_true, epoch_scores=self.epoch_scores,\n",
    "                             epoch=len(net.history), phase=phase, log_path=self.log_path)\n",
    "\n",
    "                report_lerning_process(columns=self.columns,\n",
    "                                       epoch=len(net.history),\n",
    "                                       phase=phase,\n",
    "                                       y_pred=y_pred,\n",
    "                                       y_true=y_true,\n",
    "                                       loss_history=self.loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "class MyCriterion(_Loss):\n",
    "    def __init__(self):\n",
    "        super(MyCriterion, self).__init__()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        set_trace()\n",
    "        return nn.MSELoss()(x,y)\n",
    "    \n",
    "#         set_trace()\n",
    "        \n",
    "#         valid_mask = ~(y.view(y.size(0),FRAME_MAXLEN,-1)==0).all(dim=2)\n",
    "#         valid_mask = valid_mask.float()\n",
    "#         return torch.mean(torch.sum((valid_mask * ((x-y)**2).mean((1,3,4))),1)/torch.sum(valid_mask,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for distributed computing (dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client('127.0.0.1:8786')\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search (or random search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_searchcv import GridSearchCV\n",
    "from sklearn.metrics import fbeta_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def grid_search(model, super_class,\n",
    "                task = 'regression@pretrained',\n",
    "                n_splits=2,\n",
    "                feature_extraction_model=None, feature_layer='conv1',\n",
    "                # dataset path\n",
    "                input_file = \"../../preprocess/data/person_detection_and_tracking_results_drop.pkl\",\n",
    "                target_file = \"../../preprocess/data/targets_dataframe.pkl\"):\n",
    "    \n",
    "\n",
    "    data_dict = prepare_dataset(input_file, target_file,\n",
    "                                feature_extraction_model=feature_extraction_model, layer=feature_layer)\n",
    "\n",
    "    \n",
    "    train_dataset = dataset_init(task, \n",
    "                                data_dict['train_X'], data_dict['train_y'],\n",
    "                                scaler=data_dict['scaler'], name='train')\n",
    "    \n",
    "    # holdouf test set for final evaluation\n",
    "    test_dataset = dataset_init(task, \n",
    "                                data_dict['test_X'], data_dict['test_y'],\n",
    "                                scaler=data_dict['scaler'], name='test')\n",
    "    \n",
    "    # init net\n",
    "    net = super_class(\n",
    "        model,\n",
    "        batch_size=8,\n",
    "        max_epochs=1,\n",
    "        optimizer=torch.optim.SGD,\n",
    "        device='cuda',\n",
    "        # Shuffle training data on each epoch\n",
    "        iterator_train__shuffle=True,\n",
    "        callbacks=[('ealy_stop', callbacks.EarlyStopping()),\n",
    "                   ('lr_scheduler', callbacks.LRScheduler(policy='CyclicLR',\n",
    "                                                         mode='exp_range',\n",
    "                                                         base_lr=1e-3,\n",
    "                                                         gamma=0.97)),\n",
    "                   ('MAPE', callbacks.EpochScoring(scoring=make_scorer(mape, \n",
    "                                                                       scaler = data_dict['scaler']), lower_is_better=True)),\n",
    "                   ('R2', callbacks.EpochScoring(scoring='r2', lower_is_better=False)),]\n",
    "    )\n",
    "    \n",
    "#     params = {'callbacks__lr_scheduler__base_lr': [1e-3, 1e-4],\n",
    "#               'optimizer__weight_decay': [1.0, 1e-1, 1e-2, 1e-3]},\n",
    "    params = {'callbacks__lr_scheduler__base_lr': [1e-3,],\n",
    "              'optimizer__weight_decay': [1e-1,]},\n",
    "\n",
    "    gs = GridSearchCV(net, params, refit=True, cv=n_splits, scoring='r2')\n",
    "\n",
    "    X, y_true = fetch_samples_from_dataset(train_dataset)\n",
    "    \n",
    "    gs.fit(X, y_true)\n",
    "    \n",
    "    set_trace()\n",
    "    \n",
    "    print(gs.best_estimator_, gs.best_score_, gs.best_params_)\n",
    "    \n",
    "    \n",
    "    return gs.best_estimator_, gs.best_score_, gs.best_params_, \\\n",
    "            train_dataset, test_dataset, data_dict['scaler']\n",
    "\n",
    "task = 'regression@pretrained'\n",
    "\n",
    "super_class = NeuralNetRegressor\n",
    "\n",
    "if task.split('@')[1]=='fromscratch':\n",
    "    super_class = AutoEncoderNet\n",
    "\n",
    "net, best_score, best_params, train_dataset, test_dataset, scaler = \\\n",
    "        grid_search(\n",
    "                    model=eval('_'.join(task.split('@')).capitalize()),\n",
    "                    super_class=super_class,\n",
    "                    #feature_extraction_model=tf_model, feature_layer='fc1',\n",
    "                    task=task\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation for Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_true = fetch_samples_from_dataset(test_dataset)\n",
    "y_pred = net.predict(X)\n",
    "\n",
    "if scaler:\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    y_true = scaler.inverse_transform(y_true)\n",
    "\n",
    "\n",
    "record_score(y_pred, y_true, epoch=10, phase='test', log_path='./score_log.txt')\n",
    "\n",
    "report_lerning_process(columns=target_columns,\n",
    "                       phase='test',\n",
    "                       y_pred=y_pred,\n",
    "                       y_true=y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation (base_lr = 1e-3, weight_decay = 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_loop(n_splits=5,\n",
    "                          feature_extraction_model=None, feature_layer='conv1',\n",
    "                          # dataset path\n",
    "                          input_file = \"../../preprocess/data/person_detection_and_tracking_results_drop.pkl\",\n",
    "                          target_file = \"../../preprocess/data/targets_dataframe.pkl\",\n",
    "                          callback_list=['early_stop', 'prog_bar', 'report_regression_results'],\n",
    "                          scores=['MAE', 'MAPE', 'RMSE', 'R2', 'EV']):\n",
    "    \n",
    "\n",
    "    data_dict = prepare_dataset(input_file, target_file,\n",
    "                                feature_extraction_model=feature_extraction_model, layer=feature_layer)\n",
    "\n",
    "    # holdouf test set for final evaluation\n",
    "    test_dataset = dataset_init(task, \n",
    "                                data_dict['test_X'], data_dict['test_y'],\n",
    "                                scaler=data_dict['scaler'], name='test')\n",
    "    \n",
    "    # can be modified !\n",
    "    CV_scores = { k:[] for k in scores }\n",
    "    \n",
    "    data_locations = np.array(data_dict['train_vids'])\n",
    "\n",
    "    input_df = data_dict['input_df']\n",
    "    target_df = data_dict['target_df']\n",
    "    scaler = data_dict['scaler']\n",
    "    \n",
    "    if callback_list:\n",
    "        callback_map = {'early_stop': callbacks.EarlyStopping(),\n",
    "                        'prog_bar': callbacks.ProgressBar(),\n",
    "                        'lr_scheduler': callbacks.LRScheduler(policy='CyclicLR',\n",
    "                                                                mode='exp_range',\n",
    "                                                                base_lr=1e-3,\n",
    "                                                                gamma=0.97,\n",
    "                                                               ),\n",
    "                        'report_regression_results': Report_Regression_Results(columns=target_columns, \n",
    "                                                                                 period=1,\n",
    "                                                                                 scaler=scaler,\n",
    "                                                                                 scores=scores),\n",
    "                        'save_reconstruction_results': Save_Reconstruction_Results(path='./results')\n",
    "                         }\n",
    "\n",
    "        callback_list = [ callback_map.get(cb) for cb in callback_list ]\n",
    "    \n",
    "    # K-fold CV\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    print('start!')\n",
    "    \n",
    "    for train, valid in kf.split(data_locations):\n",
    "        \n",
    "        # split trainset with train/valid\n",
    "        train_split, valid_split = data_locations[train], data_locations[valid]\n",
    "\n",
    "        train_X, train_y = filter_input_df_with_vids(input_df,train_split), filter_target_df_with_vids(target_df,train_split)\n",
    "        valid_X, valid_y = filter_input_df_with_vids(input_df,valid_split), filter_target_df_with_vids(target_df,valid_split)\n",
    "\n",
    "        # dsataset !!\n",
    "        train_dataset = dataset_init(task, train_X, train_y, scaler=scaler, name='train')\n",
    "        valid_dataset = dataset_init(task, valid_X, valid_y, scaler=scaler, name='valid')\n",
    "\n",
    "        # init net\n",
    "        net = super_class(\n",
    "            model,\n",
    "            batch_size=8,\n",
    "            max_epochs=20,\n",
    "            lr = 1e-3,\n",
    "            optimizer=torch.optim.SGD,\n",
    "            optimizer__weight_decay=1e-1,\n",
    "            device='cuda',\n",
    "            #criterion=MyCriterion,\n",
    "            #criterion__mu=train_y.values.mean(0),\n",
    "            #criterion__sigma=train_y.values.std(0),\n",
    "            train_split=predefined_split(valid_dataset),\n",
    "            # Shuffle training data on each epoch\n",
    "            iterator_train__shuffle=True,\n",
    "            warm_start=False, # re-init the module\n",
    "            callbacks=callback_list,\n",
    "        )\n",
    "        \n",
    "        # implicit train/validate loop for each CV split\n",
    "        net.fit(train_dataset, y=None)\n",
    "        \n",
    "        epoch_scores = callback_map['report_regression_results'].epoch_scores\n",
    "        for k,v in epoch_scores.items():\n",
    "            CV_scores[k].append(v)\n",
    "\n",
    "    for k,v in CV_scores.items():\n",
    "        CV_scores[k] = np.array(v).mean(0)   # avg for all CV split\n",
    "        \n",
    "    set_trace()\n",
    "    \n",
    "    return net, train_dataset, valid_dataset, test_dataset, CV_scores, scaler\n",
    "\n",
    "task = 'regression@pretrained'\n",
    "# task = 'reconstruction@pretrained'\n",
    "# task = 'reconstruction@fromscratch'\n",
    "\n",
    "super_class = NeuralNetRegressor\n",
    "if task.split('@')[1]=='fromscratch':\n",
    "    super_class = AutoEncoderNet\n",
    "\n",
    "net, train_dataset, valid_dataset, test_dataset, CV_scores, scaler = cross_validation_loop(\n",
    "    model=models_fuck.Regression_pretrained,\n",
    "    super_class=super_class,\n",
    "    #feature_extraction_model=tf_model, feature_layer='fc1',\n",
    "\n",
    "    task=task,\n",
    "    callback_list=['early_stop',\n",
    "                 'lr_scheduler',\n",
    "                 'prog_bar', 'report_regression_results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_true = fetch_samples_from_dataset(valid_dataset)\n",
    "y_pred = net.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check normality of target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.gofplots import qqplot\n",
    "qqplot(y_true[:,0], line='s'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ix,col in enumerate(target_columns):\n",
    "    sampled_y_true = scaler.inverse_transform(y_true)[:,ix] if scaler else y_true[:,ix]\n",
    "    sampled_y_pred = scaler.inverse_transform(y_pred)[:,ix] if scaler else y_pred[:,ix]\n",
    "\n",
    "    sorted_ixs = np.argsort(sampled_y_true)\n",
    "\n",
    "    sampled_y_true = sampled_y_true[sorted_ixs]\n",
    "    sampled_y_pred = sampled_y_pred[sorted_ixs]\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(2, figsize=(11,15))\n",
    "    axes[0].set_title(col+' trace')\n",
    "    axes[0].plot(sampled_y_true, 'g*')\n",
    "    axes[0].plot(sampled_y_pred)\n",
    "    axes[0].axhline(y=sampled_y_true.mean(), color='r', linestyle='--')\n",
    "    axes[1].set_title(col+' residuals')\n",
    "    axes[1].scatter(sampled_y_pred, sampled_y_pred-sampled_y_true)\n",
    "    axes[1].axhline(y=0.0, color='r', linestyle='--')\n",
    "    plt.savefig(f'figures/results/{col.replace(\"/\", \"_\")}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check normality of residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = sampled_y_pred-sampled_y_true\n",
    "plt.hist(residuals, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(residuals, line='s'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(scaler.inverse_transform(y_true), scaler.inverse_transform(y_pred), multioutput='raw_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(scaler.inverse_transform(y_true),scaler.inverse_transform(y_pred), multioutput='raw_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
