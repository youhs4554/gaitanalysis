{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from itertools import chain\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from IPython.core.debugger import set_trace\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from natsort import natsorted\n",
    "import collections\n",
    "from IPython import display\n",
    "import pylab as pl\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.helper import predefined_split\n",
    "from skorch import callbacks\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import defaultdict         \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics.regression import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.model_selection import KFold\n",
    "import c3d_wrapper\n",
    "from data_utils import *\n",
    "from models import *\n",
    "from params import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks for skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Callback\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def to_tensor_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.permute(0,3,1,2)\n",
    "    x = x.view(x.size(0), 1, 64, 64)\n",
    "    return x\n",
    "\n",
    "class Save_Reconstruction_Results(Callback):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        for name in ['train', 'valid']:\n",
    "            dataset = kwargs['dataset_'+name]\n",
    "            rand_ix = np.random.randint(len(dataset))\n",
    "            X,y = dataset[rand_ix]\n",
    "            \n",
    "            save_dir = os.path.join(self.path, name)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            \n",
    "            # target img\n",
    "            y = y.numpy().transpose(1,2,3,0)  # (maxlen,h,w,3)\n",
    "            \n",
    "            # predicted img\n",
    "            pred = net.predict(X[None,:])[0].transpose(1,2,3,0) # (maxlen,h,w,3)\n",
    "            \n",
    "            for sub_name,pic in zip(['target', 'pred'], [y,pred]):\n",
    "                pic = to_tensor_img(torch.from_numpy(pic))\n",
    "                save_image(pic, os.path.join(save_dir,sub_name+'.png'))\n",
    "\n",
    "                \n",
    "def fetch_samples_from_dataset(dataset):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for item in dataset:\n",
    "        X.append(item[0].numpy())\n",
    "        Y.append(item[1].numpy())\n",
    "        \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "class Report_Regression_Results(Callback):\n",
    "    def __init__(self, columns, scaler=None, period=5):\n",
    "        self.columns = columns\n",
    "        self.scaler = scaler\n",
    "        self.period = period\n",
    "        \n",
    "        self.loss_history = defaultdict(list)\n",
    "    \n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        if len(net.history) % self.period == 0:\n",
    "            for phase in ['train', 'valid']:\n",
    "                self.loss_history[phase].append(net.history[-1, f'{phase}_loss'])\n",
    "            \n",
    "            for phase in ['train', 'valid']:\n",
    "                dataset = kwargs['dataset_'+phase]\n",
    "\n",
    "                X, y_true = fetch_samples_from_dataset(dataset)\n",
    "                y_pred = net.predict(X)\n",
    "\n",
    "                report_lerning_process(columns=self.columns,\n",
    "                                       epoch=len(net.history),\n",
    "                                       phase=phase,\n",
    "                                       y_pred=self.scaler.inverse_transform(y_pred),\n",
    "                                       y_true=self.scaler.inverse_transform(y_true),\n",
    "                                       loss_history=self.loss_history)\n",
    "        \n",
    "    \n",
    "    def score(y_pred, y_true):\n",
    "        print('MAE :', mean_absolute_error(y_true, y_pred, multioutput='raw_values'))\n",
    "        print('MSE :', mean_squared_error(y_true, y_pred, multioutput='raw_values'))\n",
    "        print('RMSE :', np.sqrt(mean_squared_error(y_true, y_pred, multioutput='raw_values')))\n",
    "        print('R^2 : ', r2_score(y_true, y_pred, multioutput='variance_weighted'))\n",
    "        print('Explained variation : ',explained_variance_score(y_true, y_pred, multioutput='variance_weighted'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "class MyCriterion(_Loss):\n",
    "    def __init__(self):\n",
    "        super(MyCriterion, self).__init__()\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        valid_mask = ~(y.view(y.size(0),FRAME_MAXLEN,-1)==0).all(dim=2)\n",
    "        valid_mask = valid_mask.float()\n",
    "        return torch.mean(torch.sum((valid_mask * ((x-y)**2).mean((1,3,4))),1)/torch.sum(valid_mask,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_loop(model,\n",
    "                          super_class=NeuralNetRegressor,\n",
    "                          task = 'regression@pretrained',\n",
    "                          n_splits=5,\n",
    "                          feature_extraction_model=None,\n",
    "                          # dataset path\n",
    "                          input_file = \"../../preprocess/data/person_detection_and_tracking_results_drop.pkl\",\n",
    "                          target_file = \"../../preprocess/data/targets_dataframe.pkl\",\n",
    "                          callback_list=['early_stop', 'prog_bar', 'report_regression_results'],\n",
    "                          scores=['MAPE', 'MAE', 'RMSE', 'R2', 'Explained variation']):\n",
    "    \n",
    "\n",
    "    data_dict = prepare_dataset(input_file, target_file, feature_extraction_model=feature_extraction_model)\n",
    "\n",
    "    # holdouf test set for final evaluation\n",
    "    test_dataset = dataset_init(task, \n",
    "                                data_dict['test_X'], data_dict['test_y'],\n",
    "                                scaler=data_dict['scaler'], name='test')\n",
    "    \n",
    "    # can be modified !\n",
    "    scores = { k:[] for k in scores }\n",
    "    \n",
    "    data_locations = np.array(data_dict['train_vids'])\n",
    "\n",
    "    input_df = data_dict['input_df']\n",
    "    target_df = data_dict['target_df']\n",
    "    scaler = data_dict['scaler']\n",
    "    \n",
    "    if callback_list:\n",
    "        call_backs_map = {'early_stop': callbacks.EarlyStopping(),\n",
    "                          'prog_bar': callbacks.ProgressBar(),\n",
    "                          'lr_scheduler': callbacks.LRScheduler(policy='WarmRestartLR'),\n",
    "                          'report_regression_results': Report_Regression_Results(columns=target_columns, period=1, scaler=scaler),\n",
    "                          'save_reconstruction_results': Save_Reconstruction_Results(path='./results')\n",
    "                         }\n",
    "\n",
    "        callback_list = [ call_backs_map.get(cb) for cb in callback_list ]\n",
    "    \n",
    "    # K-fold CV\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    \n",
    "    for train, valid in kf.split(data_locations):\n",
    "        \n",
    "        # split trainset with train/valid\n",
    "        train_split, valid_split = data_locations[train], data_locations[valid]\n",
    "\n",
    "        train_X, train_y = filter_input_df_with_vids(input_df,train_split), filter_target_df_with_vids(target_df,train_split)\n",
    "        valid_X, valid_y = filter_input_df_with_vids(input_df,valid_split), filter_target_df_with_vids(target_df,valid_split)\n",
    "\n",
    "        # dsataset !!\n",
    "        train_dataset = dataset_init(task, train_X, train_y, scaler=scaler, name='train')\n",
    "        valid_dataset = dataset_init(task, valid_X, valid_y, scaler=scaler, name='valid')\n",
    "        \n",
    "        # init net\n",
    "        net = super_class(\n",
    "            model,\n",
    "            batch_size=10,\n",
    "            max_epochs=30,\n",
    "            lr=1e-4,\n",
    "            optimizer=torch.optim.Adam,\n",
    "            #optimizer__weight_decay=1e-4,\n",
    "            device='cuda',\n",
    "            train_split=predefined_split(valid_dataset),\n",
    "            # Shuffle training data on each epoch\n",
    "            iterator_train__shuffle=True,\n",
    "            warm_start=False, # re-init the module\n",
    "            callbacks=callback_list,\n",
    "        )\n",
    "\n",
    "        # implicit train/validate loop for each CV split\n",
    "        net.fit(train_dataset, y=None)\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize with pretrained weight file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# pretrained c3d net( tensorflow )\n",
    "tf_model = TF_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "extracting features from pretrained C3D and saving into /data/GaitData/EncodedFeatures:   0%|          | 0/2806 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/hossay/gaitanalysis/src/dev/data_utils.py\u001b[0m(80)\u001b[0;36mextract_features\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     78 \u001b[0;31m        \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extraction_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     79 \u001b[0;31m        \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 80 \u001b[0;31m        \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     81 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     82 \u001b[0;31m        \u001b[0;31m# move to next slice !\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> feature.shape\n",
      "(16, 56, 56, 64)\n",
      "ipdb> pic.shape\n",
      "*** NameError: name 'pic' is not defined\n",
      "ipdb> clip.shape\n",
      "(16, 112, 112, 3)\n",
      "ipdb> feats1 = feature[0]\n",
      "ipdb> feats1.shape\n",
      "(56, 56, 64)\n",
      "ipdb> feats1 = feature[0].transpose(2,0,1)\n",
      "ipdb> feats1.shape\n",
      "(64, 56, 56)\n",
      "ipdb> feature[0].shape\n",
      "(56, 56, 64)\n",
      "ipdb> feats1.shape\n",
      "(64, 56, 56)\n",
      "ipdb> feats1 = feature[0].transpose(2,0,1).reshape(64,1,56,56)\n",
      "ipdb> feats1.shape\n",
      "(64, 1, 56, 56)\n",
      "ipdb> feats1.min()\n",
      "0.0\n",
      "ipdb> feats1.max()\n",
      "396.81775\n",
      "ipdb> FT\n",
      "*** NameError: name 'FT' is not defined\n",
      "ipdb> import torchvision.transforms.functional as TF\n",
      "ipdb> TF.to_tensor\n",
      "<function to_tensor at 0x7f8761caf620>\n",
      "ipdb> TF.to_tensor(feats1).shape\n",
      "*** TypeError: pic should be PIL Image or ndarray. Got <class 'numpy.ndarray'>\n",
      "ipdb> TF.to_tensor(feats1)\n",
      "*** TypeError: pic should be PIL Image or ndarray. Got <class 'numpy.ndarray'>\n",
      "ipdb> feats1.shape\n",
      "(64, 1, 56, 56)\n",
      "ipdb> torch.tensor([ TF.to_tensor(f) for f in feats1 ]).shape\n",
      "*** ValueError: only one element tensors can be converted to Python scalars\n",
      "ipdb> torch.cat([ TF.to_tensor(f) for f in feats1 ]).shape\n",
      "torch.Size([3584, 1, 56])\n",
      "ipdb> feats1.shape\n",
      "(64, 1, 56, 56)\n",
      "ipdb> torch.cat([ TF.to_tensor(f) for f in feats1 ], 0).shape\n",
      "torch.Size([3584, 1, 56])\n",
      "ipdb> torch.vstack([ TF.to_tensor(f) for f in feats1 ], 0).shape\n",
      "*** AttributeError: module 'torch' has no attribute 'vstack'\n",
      "ipdb> torch.stack([ TF.to_tensor(f) for f in feats1 ], 0).shape\n",
      "torch.Size([64, 56, 1, 56])\n",
      "ipdb> TF.to_tensor(f).shape\n",
      "*** NameError: name 'f' is not defined\n",
      "ipdb> TF.to_tensor(feats1[0]).shape\n",
      "torch.Size([56, 1, 56])\n",
      "ipdb> feats1.shape\n",
      "(64, 1, 56, 56)\n",
      "ipdb> feats1 = feature[0]\n",
      "ipdb> feats1.shape\n",
      "(56, 56, 64)\n",
      "ipdb> torch.stack([ TF.to_tensor(f) for f in feats1 ], 0).shape\n",
      "torch.Size([56, 1, 56, 64])\n",
      "ipdb> feats1.shape\n",
      "(56, 56, 64)\n",
      "ipdb> TF.to_tensor(feats1).shape\n",
      "torch.Size([64, 56, 56])\n",
      "ipdb> TF.to_tensor(feats1)[:,None,:,:].shape\n",
      "torch.Size([64, 1, 56, 56])\n",
      "ipdb> res = TF.to_tensor(feats1)[:,None,:,:]\n",
      "ipdb> res\n",
      "tensor([[[[3.3739e+01, 1.1849e+01, 1.3431e+01,  ..., 2.6216e+00,\n",
      "           1.9660e+01, 0.0000e+00],\n",
      "          [3.2289e+01, 2.0834e+00, 1.3835e+01,  ..., 6.8132e+00,\n",
      "           1.6347e+01, 2.9202e+00],\n",
      "          [1.2456e+01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           1.1305e+01, 2.4219e+01],\n",
      "          ...,\n",
      "          [3.9695e+01, 6.2045e-01, 1.6841e+00,  ..., 1.2243e+00,\n",
      "           0.0000e+00, 1.9578e+00],\n",
      "          [4.6162e+01, 0.0000e+00, 1.8763e+00,  ..., 2.3413e+00,\n",
      "           0.0000e+00, 6.5369e+00],\n",
      "          [4.8989e+01, 1.1290e+00, 2.3558e+00,  ..., 8.0419e-01,\n",
      "           3.7088e+00, 1.2257e+01]]],\n",
      "\n",
      "\n",
      "        [[[8.8757e+00, 5.2395e+01, 2.2826e+01,  ..., 3.6642e+01,\n",
      "           2.1999e+01, 7.4453e+00],\n",
      "          [1.4910e+01, 4.5792e+01, 4.9823e+01,  ..., 4.6712e+01,\n",
      "           3.9535e+01, 4.7472e+00],\n",
      "          [1.7299e+01, 0.0000e+00, 5.8958e+01,  ..., 2.9121e+01,\n",
      "           9.4985e+01, 1.1964e+02],\n",
      "          ...,\n",
      "          [6.9602e+01, 0.0000e+00, 0.0000e+00,  ..., 9.4881e-01,\n",
      "           7.8278e-01, 0.0000e+00],\n",
      "          [7.9349e+01, 2.8151e-02, 3.8388e+00,  ..., 0.0000e+00,\n",
      "           6.0207e-01, 0.0000e+00],\n",
      "          [8.5624e+01, 1.5297e+01, 1.8669e+01,  ..., 1.0167e+01,\n",
      "           1.1520e+01, 5.8797e+00]]],\n",
      "\n",
      "\n",
      "        [[[0.0000e+00, 1.5486e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           1.9542e+00, 2.1966e+01],\n",
      "          [2.0411e+00, 1.1150e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           4.3462e-01, 2.0453e+01],\n",
      "          [1.4495e+00, 2.2634e+00, 1.3170e+00,  ..., 0.0000e+00,\n",
      "           4.1068e-01, 1.2496e+01],\n",
      "          ...,\n",
      "          [5.2016e-01, 7.0890e-01, 7.3431e-01,  ..., 8.0756e-01,\n",
      "           7.2009e-01, 1.3734e+00],\n",
      "          [6.1685e-01, 7.4985e-01, 9.5921e-01,  ..., 7.3393e-01,\n",
      "           9.2103e-01, 5.8055e-01],\n",
      "          [6.9690e-01, 9.3747e-01, 1.0055e+00,  ..., 1.1484e+00,\n",
      "           9.5242e-01, 8.0616e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0000e+00, 3.6597e+01, 2.5730e+00,  ..., 5.6223e+01,\n",
      "           2.3117e+01, 1.5046e+01],\n",
      "          [0.0000e+00, 1.7864e+01, 5.4980e+01,  ..., 3.0007e+01,\n",
      "           0.0000e+00, 1.8672e+01],\n",
      "          [1.2383e+01, 0.0000e+00, 5.4254e+01,  ..., 0.0000e+00,\n",
      "           8.3245e+01, 8.7938e+01],\n",
      "          ...,\n",
      "          [8.7283e+00, 1.8882e+00, 4.6932e+00,  ..., 2.4612e+00,\n",
      "           6.4154e+00, 1.3500e+00],\n",
      "          [5.9745e+00, 2.9969e+00, 4.3881e+00,  ..., 0.0000e+00,\n",
      "           7.3002e+00, 0.0000e+00],\n",
      "          [5.1248e+00, 7.7633e-01, 3.3043e+00,  ..., 7.6289e+00,\n",
      "           2.7726e+00, 1.6220e+00]]],\n",
      "\n",
      "\n",
      "        [[[1.0227e+01, 5.2082e+00, 1.8093e+01,  ..., 2.9571e+01,\n",
      "           3.9873e+01, 2.8441e+01],\n",
      "          [1.0729e+01, 0.0000e+00, 1.5676e+01,  ..., 1.4356e+01,\n",
      "           4.1388e+01, 3.8184e+01],\n",
      "          [1.4136e+01, 3.5247e+00, 0.0000e+00,  ..., 1.4362e+01,\n",
      "           1.1418e+01, 1.2988e+01],\n",
      "          ...,\n",
      "          [1.8945e+00, 7.5755e-01, 1.3672e+00,  ..., 2.4506e+00,\n",
      "           2.9193e+00, 1.1376e+01],\n",
      "          [2.7759e+00, 2.1346e+00, 6.1562e-01,  ..., 4.7809e+00,\n",
      "           3.8435e+00, 1.8188e+01],\n",
      "          [2.7521e+00, 2.2708e+00, 0.0000e+00,  ..., 9.8945e-01,\n",
      "           1.7155e+00, 1.9146e+01]]],\n",
      "\n",
      "\n",
      "        [[[2.1581e+01, 5.6188e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 5.2014e+01],\n",
      "          [1.1407e+01, 1.3028e+01, 7.2822e+00,  ..., 1.3531e+01,\n",
      "           0.0000e+00, 3.4488e+01],\n",
      "          [3.0858e+00, 9.2282e+00, 2.1352e+01,  ..., 1.0559e+01,\n",
      "           0.0000e+00, 1.3681e+01],\n",
      "          ...,\n",
      "          [2.7932e+01, 1.0036e+00, 0.0000e+00,  ..., 8.8624e-01,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [3.1543e+01, 0.0000e+00, 3.9162e-01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          [3.1647e+01, 1.1676e+00, 2.7053e+00,  ..., 1.2059e+00,\n",
      "           5.8455e-01, 1.7410e+00]]]])\n",
      "ipdb> save_image(res, 'fuck.png')\n",
      "*** NameError: name 'save_image' is not defined\n",
      "ipdb> from torchvision.utils import save_image\n",
      "ipdb> save_image(res, 'fuck.png')\n",
      "ipdb> res.min()\n",
      "tensor(0.)\n",
      "ipdb> res.max()\n",
      "tensor(396.8177)\n",
      "ipdb> feats1.shape\n",
      "(56, 56, 64)\n",
      "ipdb> feats1.min()\n",
      "0.0\n",
      "ipdb> feats1.max()\n",
      "396.81775\n",
      "ipdb> exit\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fca03d21b1a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                       \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'@'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapitalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                       \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                       callback_list=['prog_bar', 'report_regression_results'])\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-928b0eb5abb9>\u001b[0m in \u001b[0;36mcross_validation_loop\u001b[0;34m(model, super_class, task, n_splits, feature_extraction_model, input_file, target_file, callback_list, scores)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_extraction_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_extraction_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# holdouf test set for final evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gaitanalysis/src/dev/data_utils.py\u001b[0m in \u001b[0;36mprepare_dataset\u001b[0;34m(input_file, target_file, feature_extraction_model)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mpossible_vids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeature_extraction_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0msave_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpossible_vids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_extraction_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_extraction_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# split dataset (train/test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gaitanalysis/src/dev/data_utils.py\u001b[0m in \u001b[0;36msave_features\u001b[0;34m(vids, feature_extraction_model, frame_home, frames_per_clip, feats_maxlen, save_dir)\u001b[0m\n\u001b[1;32m    104\u001b[0m                                  \u001b[0mframe_home\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                                  \u001b[0mframes_per_clip\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                                  feats_maxlen)\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gaitanalysis/src/dev/data_utils.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(stacked_arr, feature_extraction_model, frame_home, frames_per_clip, feats_maxlen)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extraction_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# move to next slice !\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gaitanalysis/src/dev/data_utils.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(stacked_arr, feature_extraction_model, frame_home, frames_per_clip, feats_maxlen)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extraction_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# move to next slice !\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "task = 'regression@pretrained'\n",
    "# task = 'reconstruction@pretrained'\n",
    "# task = 'reconstruction@fromscratch'\n",
    "\n",
    "super_class = NeuralNetRegressor\n",
    "if task.split('@')[1]=='fromscratch':\n",
    "    super_class = AutoEncoderNet\n",
    "\n",
    "cross_validation_loop(super_class=super_class,\n",
    "                      feature_extraction_model=tf_model,\n",
    "                      model=eval('_'.join(task.split('@')).capitalize()),\n",
    "                      task=task,\n",
    "                      callback_list=['prog_bar', 'report_regression_results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
