{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "from itertools import chain\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from IPython.core.debugger import set_trace\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from natsort import natsorted\n",
    "import collections\n",
    "from IPython import display\n",
    "import pylab as pl\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.helper import predefined_split\n",
    "from skorch import callbacks\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import defaultdict         \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics.regression import median_absolute_error, mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.model_selection import KFold\n",
    "import c3d_wrapper\n",
    "from data_utils import *\n",
    "from models import *\n",
    "from params import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks for skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Callback\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def to_tensor_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.permute(0,3,1,2)\n",
    "    x = x.view(x.size(0), 1, 64, 64)\n",
    "    return x\n",
    "\n",
    "class Save_Reconstruction_Results(Callback):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        for name in ['train', 'valid']:\n",
    "            dataset = kwargs['dataset_'+name]\n",
    "            rand_ix = np.random.randint(len(dataset))\n",
    "            X,y = dataset[rand_ix]\n",
    "            \n",
    "            save_dir = os.path.join(self.path, name)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            \n",
    "            # target img\n",
    "            y = y.numpy().transpose(1,2,3,0)  # (maxlen,h,w,3)\n",
    "            \n",
    "            # predicted img\n",
    "            pred = net.predict(X[None,:])[0].transpose(1,2,3,0) # (maxlen,h,w,3)\n",
    "            \n",
    "            for sub_name,pic in zip(['target', 'pred'], [y,pred]):\n",
    "                pic = to_tensor_img(torch.from_numpy(pic))\n",
    "                save_image(pic, os.path.join(save_dir,sub_name+'.png'))\n",
    "\n",
    "                \n",
    "def fetch_samples_from_dataset(dataset):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for item in dataset:\n",
    "        X.append(item[0].numpy())\n",
    "        Y.append(item[1].numpy())\n",
    "        \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def record_score(y_pred, y_true, epoch, phase, log_path=\"score_log.txt\"):\n",
    "    deco = f\"=== EPOCH-{epoch} / {phase} === summary\"\n",
    "    mae = f\"MAE : {mean_absolute_error(y_true, y_pred)}\"\n",
    "    mse = f\"MSE : {mean_squared_error(y_true, y_pred)}\"\n",
    "    rmse = f\"RMSE : {np.sqrt(mean_squared_error(y_true, y_pred))}\"\n",
    "    r2 = f\"R^2 : {r2_score(y_true, y_pred, multioutput='variance_weighted')}\"\n",
    "    ev = f\"Explained variation : {explained_variance_score(y_true, y_pred, multioutput='variance_weighted')}\"\n",
    "    \n",
    "    msg = '\\n'.join([deco, mae, mse, rmse, r2, ev])\n",
    "    os.system(\"echo \\'{}\\' >> {}\".format(msg, log_path))\n",
    "    \n",
    "\n",
    "class Report_Regression_Results(Callback):\n",
    "    def __init__(self, columns, scaler=None, period=5, log_path=\"score_log.txt\"):\n",
    "        self.columns = columns\n",
    "        self.scaler = scaler\n",
    "        self.period = period\n",
    "        \n",
    "        self.loss_history = defaultdict(list)\n",
    "        if os.path.exists(log_path):\n",
    "            os.system(f\"rm {log_path}\")\n",
    "            \n",
    "        self.log_path = log_path\n",
    "    \n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        if len(net.history) % self.period == 0:\n",
    "            for phase in ['train', 'valid']:\n",
    "                self.loss_history[phase].append(net.history[-1, f'{phase}_loss'])\n",
    "            \n",
    "            for phase in ['train', 'valid']:\n",
    "                dataset = kwargs['dataset_'+phase]\n",
    "\n",
    "                X, y_true = fetch_samples_from_dataset(dataset)\n",
    "                y_pred = net.predict(X)\n",
    "                if self.scaler:\n",
    "                    y_pred = self.scaler.inverse_transform(y_pred)\n",
    "                    y_true = self.scaler.inverse_transform(y_true)\n",
    "                \n",
    "                record_score(y_pred, y_true, epoch=len(net.history), phase=phase, log_path=self.log_path)\n",
    "\n",
    "                report_lerning_process(columns=self.columns,\n",
    "                                       epoch=len(net.history),\n",
    "                                       phase=phase,\n",
    "                                       y_pred=y_pred,\n",
    "                                       y_true=y_true,\n",
    "                                       loss_history=self.loss_history)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normal(x, mu=0.0, sigma=1.0):\n",
    "    if type(x)==torch.Tensor:\n",
    "        x = x.detach().cpu().numpy()\n",
    "        \n",
    "    res = (1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (x - mu)**2 / (2 * sigma**2) )).astype(np.float32)\n",
    "    return torch.tensor(res).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "class MyCriterion(_Loss):\n",
    "    def __init__(self):\n",
    "        super(MyCriterion, self).__init__()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        set_trace()\n",
    "        return nn.MSELoss()(x,y)\n",
    "    \n",
    "#         set_trace()\n",
    "        \n",
    "#         valid_mask = ~(y.view(y.size(0),FRAME_MAXLEN,-1)==0).all(dim=2)\n",
    "#         valid_mask = valid_mask.float()\n",
    "#         return torch.mean(torch.sum((valid_mask * ((x-y)**2).mean((1,3,4))),1)/torch.sum(valid_mask,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hossay/anaconda3/envs/torch/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_loop(model,\n",
    "                          super_class=NeuralNetRegressor,\n",
    "                          task = 'regression@pretrained',\n",
    "                          n_splits=5,\n",
    "                          feature_extraction_model=None, feature_layer='conv1',\n",
    "                          # dataset path\n",
    "                          input_file = \"../../preprocess/data/person_detection_and_tracking_results_drop.pkl\",\n",
    "                          target_file = \"../../preprocess/data/targets_dataframe.pkl\",\n",
    "                          callback_list=['early_stop', 'prog_bar', 'report_regression_results'],\n",
    "                          scores=['MAPE', 'MAE', 'RMSE', 'R2', 'Explained variation']):\n",
    "    \n",
    "\n",
    "    data_dict = prepare_dataset(input_file, target_file,\n",
    "                                feature_extraction_model=feature_extraction_model, layer=feature_layer)\n",
    "\n",
    "    # holdouf test set for final evaluation\n",
    "    test_dataset = dataset_init(task, \n",
    "                                data_dict['test_X'], data_dict['test_y'],\n",
    "                                scaler=data_dict['scaler'], name='test')\n",
    "    \n",
    "    # can be modified !\n",
    "    scores = { k:[] for k in scores }\n",
    "    \n",
    "    data_locations = np.array(data_dict['train_vids'])\n",
    "\n",
    "    input_df = data_dict['input_df']\n",
    "    target_df = data_dict['target_df']\n",
    "    scaler = data_dict['scaler']\n",
    "    \n",
    "    if callback_list:\n",
    "        call_backs_map = {'early_stop': callbacks.EarlyStopping(),\n",
    "                          'prog_bar': callbacks.ProgressBar(),\n",
    "                          'lr_scheduler': callbacks.LRScheduler(policy='WarmRestartLR'),\n",
    "                          'report_regression_results': Report_Regression_Results(columns=target_columns, period=1, scaler=scaler),\n",
    "                          'save_reconstruction_results': Save_Reconstruction_Results(path='./results')\n",
    "                         }\n",
    "\n",
    "        callback_list = [ call_backs_map.get(cb) for cb in callback_list ]\n",
    "    \n",
    "    # K-fold CV\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    \n",
    "    for train, valid in kf.split(data_locations):\n",
    "        \n",
    "        # split trainset with train/valid\n",
    "        train_split, valid_split = data_locations[train], data_locations[valid]\n",
    "\n",
    "        train_X, train_y = filter_input_df_with_vids(input_df,train_split), filter_target_df_with_vids(target_df,train_split)\n",
    "        valid_X, valid_y = filter_input_df_with_vids(input_df,valid_split), filter_target_df_with_vids(target_df,valid_split)\n",
    "\n",
    "        # dsataset !!\n",
    "        train_dataset = dataset_init(task, train_X, train_y, scaler=scaler, name='train')\n",
    "        valid_dataset = dataset_init(task, valid_X, valid_y, scaler=scaler, name='valid')\n",
    "\n",
    "        # init net\n",
    "        net = super_class(\n",
    "            model,\n",
    "            batch_size=8,\n",
    "            max_epochs=30,\n",
    "            lr = 1e-4,\n",
    "            optimizer=torch.optim.SGD,\n",
    "            optimizer__weight_decay=1.0,\n",
    "            device='cuda',\n",
    "            #criterion=MyCriterion,\n",
    "            #criterion__mu=train_y.values.mean(0),\n",
    "            #criterion__sigma=train_y.values.std(0),\n",
    "            train_split=predefined_split(valid_dataset),\n",
    "            # Shuffle training data on each epoch\n",
    "            iterator_train__shuffle=True,\n",
    "            warm_start=False, # re-init the module\n",
    "            callbacks=callback_list,\n",
    "        )\n",
    "        \n",
    "        with joblib.parallel_backend('dask', scheduler_host='127.0.0.1:8786'):\n",
    "            # implicit train/validate loop for each CV split\n",
    "            net.fit(train_dataset, y=None)\n",
    "                    \n",
    "        return net, train_dataset, valid_dataset, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize with pretrained weight file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# pretrained c3d net( tensorflow )\n",
    "tf_model = TF_Model()\n",
    "\n",
    "#visualize_conv_featsmap('7157030_test_0_trial_1', tf_model, layer='conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=238), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1      \u001b[36m191.9649\u001b[0m       \u001b[32m41.3965\u001b[0m  9.5880\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=238), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "task = 'regression@pretrained'\n",
    "# task = 'reconstruction@pretrained'\n",
    "# task = 'reconstruction@fromscratch'\n",
    "\n",
    "super_class = NeuralNetRegressor\n",
    "if task.split('@')[1]=='fromscratch':\n",
    "    super_class = AutoEncoderNet\n",
    "\n",
    "net, train_dataset, valid_dataset, scaler = cross_validation_loop(super_class=super_class,\n",
    "                      #feature_extraction_model=tf_model, feature_layer='fc1',\n",
    "                      model=eval('_'.join(task.split('@')).capitalize()),\n",
    "                      task=task,\n",
    "                      callback_list=['early_stop', 'prog_bar', 'report_regression_results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_true = fetch_samples_from_dataset(valid_dataset)\n",
    "y_pred = net.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check normality of target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.gofplots import qqplot\n",
    "qqplot(y_true[:,0], line='s'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_y_true = scaler.inverse_transform(y_true)[:,-1] if scaler else y_true[:,-1]\n",
    "sampled_y_pred = scaler.inverse_transform(y_pred)[:,-1] if scaler else y_pred[:,-1]\n",
    "\n",
    "sorted_ixs = np.argsort(sampled_y_true)\n",
    "\n",
    "sampled_y_true = sampled_y_true[sorted_ixs]\n",
    "sampled_y_pred = sampled_y_pred[sorted_ixs]\n",
    "\n",
    "fig, axes = plt.subplots(2, figsize=(11,15))\n",
    "axes[0].plot(sampled_y_true, 'g*')\n",
    "axes[0].plot(sampled_y_pred)\n",
    "axes[1].scatter(sampled_y_pred, sampled_y_pred-sampled_y_true)\n",
    "axes[1].axhline(y=0.0, color='r', linestyle='-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check normality of residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = sampled_y_pred-sampled_y_true\n",
    "plt.hist(residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(residuals, line='s'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true, y_pred, multioutput='raw_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_true, y_pred, multioutput='raw_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
