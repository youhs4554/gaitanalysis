{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "\n",
    "from itertools import chain\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from IPython.core.debugger import set_trace\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "from natsort import natsorted\n",
    "import collections\n",
    "from IPython import display\n",
    "import pylab as pl\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.helper import predefined_split\n",
    "from skorch import callbacks\n",
    "from collections import defaultdict         \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics.regression import median_absolute_error, mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.model_selection import KFold\n",
    "import c3d_wrapper\n",
    "from data_utils import *\n",
    "import models\n",
    "from params import *\n",
    "from statsmodels.graphics.gofplots import qqplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF model for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained c3d net( tensorflow )\n",
    "# tf_model = TF_Model()\n",
    "\n",
    "#visualize_conv_featsmap('7157030_test_0_trial_1', tf_model, layer='conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Callback\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def to_tensor_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.permute(0,3,1,2)\n",
    "    x = x.view(x.size(0), 1, 64, 64)\n",
    "    return x\n",
    "\n",
    "class Save_Reconstruction_Results(Callback):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        for name in ['train', 'valid']:\n",
    "            dataset = kwargs['dataset_'+name]\n",
    "            rand_ix = np.random.randint(len(dataset))\n",
    "            X,y = dataset[rand_ix]\n",
    "            \n",
    "            save_dir = os.path.join(self.path, name)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            \n",
    "            # target img\n",
    "            y = y.numpy().transpose(1,2,3,0)  # (maxlen,h,w,3)\n",
    "            \n",
    "            # predicted img\n",
    "            pred = net.predict(X[None,:])[0].transpose(1,2,3,0) # (maxlen,h,w,3)\n",
    "            \n",
    "            for sub_name,pic in zip(['target', 'pred'], [y,pred]):\n",
    "                pic = to_tensor_img(torch.from_numpy(pic))\n",
    "                save_image(pic, os.path.join(save_dir,sub_name+'.png'))\n",
    "\n",
    "                \n",
    "def fetch_samples_from_dataset(dataset):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for item in dataset:\n",
    "        X.append(item[0].numpy())\n",
    "        Y.append(item[1].numpy())\n",
    "        \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def check_array(arr):\n",
    "    dim = arr.shape[1]\n",
    "    \n",
    "    # select non-nan rows\n",
    "    arr = arr[(~np.isnan(arr)).all(1)].reshape(-1, dim)    \n",
    "    \n",
    "    # select non-inf rows\n",
    "    arr = arr[(arr!=np.inf).all(1)]\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def mape(y_true, y_pred, scaler=None, reduce_all=True):\n",
    "    if scaler:\n",
    "        # when skorch callbacks is executed for epoch_scoring\n",
    "        y_pred = scaler.inverse_transform(y_pred)\n",
    "        y_true = scaler.inverse_transform(y_true)\n",
    "        \n",
    "    with np.errstate(divide='ignore'):\n",
    "        mape = np.abs((y_true-y_pred)/y_true)\n",
    "\n",
    "    if reduce_all:\n",
    "        return check_array(mape).mean()\n",
    "    else:\n",
    "        return check_array(mape).mean(0)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred, reduce_all=True):\n",
    "    if reduce_all:\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    else:\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred, multioutput='raw_values'))\n",
    "\n",
    "def record_score(y_pred, y_true, phase, save_path=\"./scores\"):\n",
    "    MAE = mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
    "    MAPE = mape(y_true, y_pred, reduce_all=False)\n",
    "    RMSE = root_mean_squared_error(y_true, y_pred, reduce_all=False)\n",
    "    R2 = r2_score(y_true, y_pred, multioutput='raw_values')\n",
    "    EV = explained_variance_score(y_true, y_pred, multioutput='raw_values')\n",
    "    \n",
    "    mae_ = f\"MAE : {MAE}\"\n",
    "    mape_ = f\"MAPE : {MAPE}\"\n",
    "    rmse_ = f\"RMSE : {RMSE}\"\n",
    "    r2_ = f\"R^2 : {R2}\"\n",
    "    ev_ = f\"Explained variation : {EV}\"\n",
    "    \n",
    "    msg = '\\n'.join([mae_, mape_, rmse_, r2_, ev_])\n",
    "    \n",
    "    log_path = os.path.join(save_path, f'{phase}.txt')\n",
    "    \n",
    "    os.system(f\"mkdir -p {save_path}\")\n",
    "    os.system(\"echo \\'{}\\' > {}\".format(msg, log_path))\n",
    "    \n",
    "    print(f'resulting scores have been saved at \\\"{log_path}\\\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "class MyCriterion(_Loss):\n",
    "    def __init__(self):\n",
    "        super(MyCriterion, self).__init__()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        set_trace()\n",
    "        return nn.MSELoss()(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup for distributed computing (dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client('127.0.0.1:8786',\n",
    "               serializers=['dask', 'pickle'],\n",
    "                deserializers=['dask', 'msgpack'])\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search (random search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_searchcv import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from skorch.callbacks import Checkpoint, TrainEndCheckpoint\n",
    "\n",
    "# for random number generation\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from scipy.stats import randint as sp_randint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hyperparams_search(model, super_class, \n",
    "                       criterion=nn.modules.loss.MSELoss,\n",
    "                       ckpt_dir=None, n_splits=5, train_dataset=None, scaler=None):\n",
    "    \n",
    "    # init net\n",
    "    net = super_class(\n",
    "        model,\n",
    "        max_epochs=30,\n",
    "        optimizer=torch.optim.SGD,\n",
    "        device='cuda',\n",
    "        criterion=criterion,\n",
    "        # Shuffle training data on each epoch\n",
    "        iterator_train__shuffle=True,\n",
    "        callbacks=[('ealy_stop', callbacks.EarlyStopping()),\n",
    "                   ('lr_scheduler', callbacks.LRScheduler(policy='CyclicLR',\n",
    "                                                         mode='exp_range',\n",
    "                                                         base_lr=1e-3,\n",
    "                                                         gamma=0.97)),\n",
    "                   ('MAPE', callbacks.EpochScoring(scoring=make_scorer(mape, \n",
    "                                                                       scaler = data_dict['scaler']), lower_is_better=True)),\n",
    "                   ('R2', callbacks.EpochScoring(scoring='r2', lower_is_better=False)),\n",
    "                   ]\n",
    "    )\n",
    "\n",
    "    params = {'batch_size': sp_randint(4,12),\n",
    "              'callbacks__lr_scheduler__base_lr': sp_uniform(loc=1e-6, scale=1e-2-1e-6),\n",
    "              'callbacks__lr_scheduler__gamma': sp_uniform(loc=0.5, scale=1.0-0.5),\n",
    "              'optimizer__weight_decay': sp_uniform(loc=0.0, scale=1e-2),\n",
    "              'module__num_units': sp_randint(64,256),\n",
    "              'module__drop_rate': sp_uniform(loc=0.0, scale=0.5),\n",
    "             }\n",
    "    \n",
    "    search = RandomizedSearchCV(net, params, refit=True, cv=n_splits,n_iter=20, iid=False,\n",
    "                                scoring=make_scorer(r2_score, multioutput='variance_weighted'))\n",
    "    \n",
    "    X, y_true = fetch_samples_from_dataset(train_dataset)\n",
    "    \n",
    "    search.fit(X, y_true)  \n",
    "    \n",
    "    os.system(f'mkdir -p {ckpt_dir}')\n",
    "    \n",
    "    print(f'save best model to {ckpt_dir}...!')\n",
    "    \n",
    "    search.best_estimator_.save_params(\n",
    "        f_params=os.path.join(ckpt_dir, 'params.pt'),\n",
    "        f_optimizer=os.path.join(ckpt_dir, 'optimizer.pt'),\n",
    "        f_history=os.path.join(ckpt_dir, 'history.json'))\n",
    "    \n",
    "    # save best params also !\n",
    "    json.dump(search.best_params_, open(os.path.join(ckpt_dir, 'best_params.json'), 'w'))\n",
    "    \n",
    "    return search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'regression@pretrained'\n",
    "\n",
    "input_file = \"../../preprocess/data/person_detection_and_tracking_results_drop.pkl\"\n",
    "target_file = \"../../preprocess/data/targets_dataframe.pkl\"\n",
    "\n",
    "feature_extraction_model=None\n",
    "feature_layer='conv1'\n",
    "\n",
    "data_dict = prepare_dataset(input_file, target_file,\n",
    "                            feature_extraction_model=feature_extraction_model, layer=feature_layer)\n",
    "\n",
    "train_dataset = dataset_init(task,\n",
    "                            data_dict['train_X'], data_dict['train_y'],\n",
    "                            scaler=data_dict['scaler'], name='train')\n",
    "\n",
    "# holdouf testset for final evaluation\n",
    "test_dataset = dataset_init(task, \n",
    "                            data_dict['test_X'], data_dict['test_y'],\n",
    "                            scaler=data_dict['scaler'], name='test')\n",
    "\n",
    "scaler = data_dict['scaler']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training configurations & run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "super_class = NeuralNetRegressor\n",
    "\n",
    "if task.split('@')[1]=='fromscratch':\n",
    "    super_class = AutoEncoderNet\n",
    "\n",
    "experiments_dict = {}\n",
    "\n",
    "for name,cr in zip(['MSE', 'MAE', 'Huber'],[nn.modules.loss.MSELoss, nn.modules.loss.L1Loss, nn.modules.loss.SmoothL1Loss]):\n",
    "    search = hyperparams_search(\n",
    "                            model=eval('_'.join(task.split('@')).capitalize()),\n",
    "                            super_class=super_class, criterion=cr,\n",
    "                            ckpt_dir=f'./models/exp_with_{name}_cost',\n",
    "                            train_dataset=train_dataset, scaler=scaler)\n",
    "    experiments_dict[name] = search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation for Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def visualize_trend(y_true, y_pred, save_dir):\n",
    "    for ix,col in enumerate(target_columns):\n",
    "        sampled_y_true = y_true[:,ix]\n",
    "        sampled_y_pred = y_pred[:,ix]\n",
    "\n",
    "        sorted_ixs = np.argsort(sampled_y_true)\n",
    "\n",
    "        sampled_y_true = sampled_y_true[sorted_ixs]\n",
    "        sampled_y_pred = sampled_y_pred[sorted_ixs]\n",
    "\n",
    "        fig, axes = plt.subplots(2, figsize=(11,15))\n",
    "        axes[0].set_title(col+' trace')\n",
    "        axes[0].plot(sampled_y_true, 'g*')\n",
    "        axes[0].plot(sampled_y_pred)\n",
    "        axes[0].axhline(y=sampled_y_true.mean(), color='r', linestyle='--')\n",
    "        axes[1].set_title(col+' residuals')\n",
    "        axes[1].scatter(sampled_y_pred, sampled_y_pred-sampled_y_true)\n",
    "        axes[1].axhline(y=0.0, color='r', linestyle='--')\n",
    "        \n",
    "        os.system(f'mkdir -p {save_dir}')\n",
    "        plt.savefig(os.path.join(save_dir, f'{col.replace(\"/\", \"_\")}.png'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_performance(test_dataset, phase, ckpt_dir, score_dir, figure_dir, scaler=None):\n",
    "    \n",
    "    f_best_params = os.path.join(ckpt_dir, 'best_params.json')\n",
    "    f_params = os.path.join(ckpt_dir, 'params.pt')\n",
    "    \n",
    "    # best hyperparams dict\n",
    "    best_params = json.load(open(f_best_params))\n",
    "    \n",
    "    # restore net from ckpt\n",
    "    net = NeuralNetRegressor(\n",
    "        module=Regression_pretrained,\n",
    "        device='cuda',\n",
    "        **best_params,\n",
    "    )\n",
    "    net.initialize()  \n",
    "    net.load_params(f_params=f_params)\n",
    "    \n",
    "    X, y_true = fetch_samples_from_dataset(test_dataset)\n",
    "    y_pred = net.predict(X)\n",
    "\n",
    "    if scaler:\n",
    "        y_pred = scaler.inverse_transform(y_pred)\n",
    "        y_true = scaler.inverse_transform(y_true)\n",
    "\n",
    "    record_score(y_pred, y_true, phase=phase, save_path=score_dir)\n",
    "\n",
    "    report_lerning_process(columns=target_columns,\n",
    "                           phase=phase, save_dir=figure_dir,\n",
    "                           y_pred=y_pred,\n",
    "                           y_true=y_true)\n",
    "    \n",
    "    visualize_trend(y_true, y_pred, save_dir=figure_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in ['MSE', 'MAE', 'Huber']:\n",
    "    measure_performance(test_dataset, phase='test', \n",
    "                         ckpt_dir=f'./models/exp_with_{name}_cost',\n",
    "                         score_dir=f'./scores/exp_with_{name}_cost',\n",
    "                         figure_dir=f'./figures/results/exp_with_{name}_cost',\n",
    "                         scaler=scaler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
