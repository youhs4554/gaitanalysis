{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from itertools import chain\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from IPython.core.debugger import set_trace\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from natsort import natsorted\n",
    "import collections\n",
    "from IPython import display\n",
    "import pylab as pl\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.helper import predefined_split\n",
    "from skorch import callbacks\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import defaultdict         \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics.regression import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.model_selection import KFold\n",
    "import c3d_wrapper\n",
    "from data_utils import *\n",
    "from models import *\n",
    "from params import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks for skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Callback\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def to_tensor_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.permute(0,3,1,2)\n",
    "    x = x.view(x.size(0), 1, 64, 64)\n",
    "    return x\n",
    "\n",
    "class Save_Reconstruction_Results(Callback):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        for name in ['train', 'valid']:\n",
    "            dataset = kwargs['dataset_'+name]\n",
    "            rand_ix = np.random.randint(len(dataset))\n",
    "            X,y = dataset[rand_ix]\n",
    "            \n",
    "            save_dir = os.path.join(self.path, name)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            \n",
    "            # target img\n",
    "            y = y.numpy().transpose(1,2,3,0)  # (maxlen,h,w,3)\n",
    "            \n",
    "            # predicted img\n",
    "            pred = net.predict(X[None,:])[0].transpose(1,2,3,0) # (maxlen,h,w,3)\n",
    "            \n",
    "            for sub_name,pic in zip(['target', 'pred'], [y,pred]):\n",
    "                pic = to_tensor_img(torch.from_numpy(pic))\n",
    "                save_image(pic, os.path.join(save_dir,sub_name+'.png'))\n",
    "\n",
    "                \n",
    "def fetch_samples_from_dataset(dataset):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for item in dataset:\n",
    "        X.append(item[0].numpy())\n",
    "        Y.append(item[1].numpy())\n",
    "        \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "class Report_Regression_Results(Callback):\n",
    "    def __init__(self, columns, scaler=None, period=5):\n",
    "        self.columns = columns\n",
    "        self.scaler = scaler\n",
    "        self.period = period\n",
    "        \n",
    "        self.loss_history = defaultdict(list)\n",
    "    \n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        if len(net.history) % self.period == 0:\n",
    "            for phase in ['train', 'valid']:\n",
    "                self.loss_history[phase].append(net.history[-1, f'{phase}_loss'])\n",
    "            \n",
    "            for phase in ['train', 'valid']:\n",
    "                dataset = kwargs['dataset_'+phase]\n",
    "\n",
    "                X, y_true = fetch_samples_from_dataset(dataset)\n",
    "                y_pred = net.predict(X)\n",
    "\n",
    "                report_lerning_process(columns=self.columns,\n",
    "                                       epoch=len(net.history),\n",
    "                                       phase=phase,\n",
    "                                       y_pred=self.scaler.inverse_transform(y_pred),\n",
    "                                       y_true=self.scaler.inverse_transform(y_true),\n",
    "                                       loss_history=self.loss_history)\n",
    "        \n",
    "    \n",
    "    def score(y_pred, y_true):\n",
    "        print('MAE :', mean_absolute_error(y_true, y_pred, multioutput='raw_values'))\n",
    "        print('MSE :', mean_squared_error(y_true, y_pred, multioutput='raw_values'))\n",
    "        print('RMSE :', np.sqrt(mean_squared_error(y_true, y_pred, multioutput='raw_values')))\n",
    "        print('R^2 : ', r2_score(y_true, y_pred, multioutput='variance_weighted'))\n",
    "        print('Explained variation : ',explained_variance_score(y_true, y_pred, multioutput='variance_weighted'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "class MyCriterion(_Loss):\n",
    "    def __init__(self):\n",
    "        super(MyCriterion, self).__init__()\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        valid_mask = ~(y.view(y.size(0),FRAME_MAXLEN,-1)==0).all(dim=2)\n",
    "        valid_mask = valid_mask.float()\n",
    "        return torch.mean(torch.sum((valid_mask * ((x-y)**2).mean((1,3,4))),1)/torch.sum(valid_mask,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_loop(model,\n",
    "                          super_class=NeuralNetRegressor,\n",
    "                          task = 'regression@pretrained',\n",
    "                          n_splits=5,\n",
    "                          feature_extraction_model=None,\n",
    "                          # dataset path\n",
    "                          input_file = \"../preprocess/data/person_detection_and_tracking_results_drop.pkl\",\n",
    "                          target_file = \"../preprocess/data/targets_dataframe.pkl\",\n",
    "                          callback_list=['early_stop', 'prog_bar', 'report_regression_results'],\n",
    "                          scores=['MAPE', 'MAE', 'RMSE', 'R2', 'Explained variation']):\n",
    "    \n",
    "\n",
    "    data_dict = prepare_dataset(input_file, target_file, feature_extraction_model=feature_extraction_model)\n",
    "\n",
    "    # holdouf test set for final evaluation\n",
    "    test_dataset = dataset_init(task, \n",
    "                                data_dict['test_X'], data_dict['test_y'],\n",
    "                                scaler=data_dict['scaler'], name='test')\n",
    "    \n",
    "    # can be modified !\n",
    "    scores = { k:[] for k in scores }\n",
    "    \n",
    "    data_locations = np.array(data_dict['train_vids'])\n",
    "\n",
    "    input_df = data_dict['input_df']\n",
    "    target_df = data_dict['target_df']\n",
    "    scaler = data_dict['scaler']\n",
    "    \n",
    "    if callback_list:\n",
    "        call_backs_map = {'early_stop': callbacks.EarlyStopping(),\n",
    "                          'prog_bar': callbacks.ProgressBar(),\n",
    "                          'lr_scheduler': callbacks.LRScheduler(policy='WarmRestartLR'),\n",
    "                          'report_regression_results': Report_Regression_Results(columns=target_columns, period=1, scaler=scaler),\n",
    "                          'save_reconstruction_results': Save_Reconstruction_Results(path='./results')\n",
    "                         }\n",
    "\n",
    "        callback_list = [ call_backs_map.get(cb) for cb in callback_list ]\n",
    "    \n",
    "    # K-fold CV\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    \n",
    "    for train, valid in kf.split(data_locations):\n",
    "        \n",
    "        # split trainset with train/valid\n",
    "        train_split, valid_split = data_locations[train], data_locations[valid]\n",
    "\n",
    "        train_X, train_y = filter_input_df_with_vids(input_df,train_split), filter_target_df_with_vids(target_df,train_split)\n",
    "        valid_X, valid_y = filter_input_df_with_vids(input_df,valid_split), filter_target_df_with_vids(target_df,valid_split)\n",
    "\n",
    "        # dsataset !!\n",
    "        train_dataset = dataset_init(task, train_X, train_y, scaler=scaler, name='train')\n",
    "        valid_dataset = dataset_init(task, valid_X, valid_y, scaler=scaler, name='valid')\n",
    "        \n",
    "        # init net\n",
    "        net = super_class(\n",
    "            model,\n",
    "            batch_size=10,\n",
    "            max_epochs=30,\n",
    "            lr=1e-4,\n",
    "            optimizer=torch.optim.Adam,\n",
    "            #optimizer__weight_decay=1e-4,\n",
    "            device='cuda',\n",
    "            train_split=predefined_split(valid_dataset),\n",
    "            # Shuffle training data on each epoch\n",
    "            iterator_train__shuffle=True,\n",
    "            warm_start=False, # re-init the module\n",
    "            callbacks=callback_list,\n",
    "        )\n",
    "\n",
    "        # implicit train/validate loop for each CV split\n",
    "        net.fit(train_dataset, y=None)\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained c3d net( tensorflow )\n",
    "tf_model = TF_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task = 'regression@pretrained'\n",
    "# task = 'reconstruction@pretrained'\n",
    "# task = 'reconstruction@fromscratch'\n",
    "\n",
    "super_class = NeuralNetRegressor\n",
    "if task.split('@')[1]=='fromscratch':\n",
    "    super_class = AutoEncoderNet\n",
    "\n",
    "cross_validation_loop(super_class=super_class,\n",
    "                      feature_extraction_model=tf_model,\n",
    "                      model=eval('_'.join(task.split('@')).capitalize()),\n",
    "                      task=task,\n",
    "                      callback_list=['prog_bar', 'report_regression_results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
