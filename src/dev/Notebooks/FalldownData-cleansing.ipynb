{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import natsort\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data 1 : URFD\n",
    "- step 1 : pack each directory containing .png files as .avi file (to follow UCF format)\n",
    "- step 2 : re-arange video files according to the class label\n",
    "- step 3 : copy-and-paste UCF clip generation code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:32<00:00,  4.55it/s]\n"
     ]
    }
   ],
   "source": [
    "root = '/data/FallDownData/URFD/raw'\n",
    "video_files = natsort.natsorted(glob.glob(root + '/*/*'))\n",
    "for video in tqdm(video_files):\n",
    "    prefix, ext = os.path.splitext(video)\n",
    "    save_path = prefix.replace('raw', 'video') + '.avi'\n",
    "    \n",
    "    frames = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(video)\n",
    "    frame_height, frame_width = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)), int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    \n",
    "    out = cv2.VideoWriter(save_path,\n",
    "                          cv2.VideoWriter_fourcc('M','J','P','G'), \n",
    "                          30, (frame_width//2,frame_height))\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # left : depth, right : color\n",
    "        color_frame = frame[:, frame_width//2:]\n",
    "        out.write(color_frame)\n",
    "\n",
    "    os.system('mkdir -p {}'.format(os.path.dirname(save_path)))\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data 2 : Multicam FDD\n",
    "- this dataset is bullshit!\n",
    "- syncronization is not matched well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/data/FallDownData/MulticamFD/raw'\n",
    "df = pd.read_csv(os.path.join(os.path.dirname(root), 'Multicam_Annotations.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arange_multicam_dataset(root):\n",
    "    videos = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        for fn in filenames:\n",
    "            name, ext = os.path.splitext(fn)\n",
    "            if ext == '.avi':\n",
    "                videos.append(os.path.join(dirpath, fn))\n",
    "\n",
    "\n",
    "    videos = natsort.natsorted(videos)\n",
    "    scenarios = np.unique([os.path.dirname(p) for p in videos])\n",
    "    \n",
    "    for scenario in tqdm(scenarios):\n",
    "        def get_shortest_frame_count():\n",
    "\n",
    "            shortest_frame_count = np.inf\n",
    "\n",
    "            for i in range(1,9):\n",
    "                vpath = os.path.join(scenario, f'cam{i}.avi')\n",
    "                cap = cv2.VideoCapture(vpath)\n",
    "                frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "                if frame_count < shortest_frame_count:\n",
    "                    shortest_frame_count = frame_count\n",
    "\n",
    "            shortest_frame_count = int(shortest_frame_count)\n",
    "\n",
    "            return shortest_frame_count\n",
    "\n",
    "        shortest_frame_count = get_shortest_frame_count()\n",
    "\n",
    "        def get_syncronized_videos():\n",
    "            res = []\n",
    "            for i in range(1,9):\n",
    "                vpath = os.path.join(scenario, f'cam{i}.avi')\n",
    "                cap = cv2.VideoCapture(vpath)\n",
    "                frames = []\n",
    "                while True:\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    frames.append(frame)\n",
    "\n",
    "                res.append(frames[-shortest_frame_count:])\n",
    "\n",
    "            return np.array(res)\n",
    "\n",
    "        syncronized = get_syncronized_videos()\n",
    "\n",
    "        scenario_id = eval(os.path.basename(scenario).split('chute')[1].lstrip('0'))\n",
    "        scenario_video_home = os.path.dirname(scenario).replace('raw', 'video')\n",
    "\n",
    "        for seg_ix, (start, end, code) in enumerate(df[df.id==scenario_id][[\"start\",\"end\",\"code\"]].values):\n",
    "            segment = syncronized[:, start:end]  # a frame segment\n",
    "\n",
    "            label = 'fall' if code == 2 else 'adl'\n",
    "            scenario_video = os.path.join(scenario_video_home, label)\n",
    "\n",
    "            os.system('mkdir -p {}'.format(scenario_video))\n",
    "\n",
    "            for i in range(1,9):\n",
    "                seg_ix_str = str(seg_ix+1).zfill(2)\n",
    "                vpath = os.path.join(scenario, f'cam{i}.avi')\n",
    "                scenario_indicator = os.path.basename(scenario)\n",
    "\n",
    "                seg_vpath = os.path.join(scenario_video, f'{scenario_indicator}-cam{i}-s{seg_ix_str}.avi')\n",
    "\n",
    "                frame_height, frame_width = segment[i-1][0].shape[:2]\n",
    "\n",
    "                fps = cv2.VideoCapture(vpath).get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "                out = cv2.VideoWriter(seg_vpath,\n",
    "                                  cv2.VideoWriter_fourcc('M','J','P','G'), \n",
    "                                  fps, (frame_width,frame_height))\n",
    "\n",
    "                for frame in segment[i-1]:\n",
    "                    out.write(frame)\n",
    "\n",
    "                    \n",
    "arange_multicam_dataset(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('/data/FallDownData/MulticamFD/video/fall/chute09-cam6-s05.avi')\n",
    "\n",
    "frames = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frames.append(frame)\n",
    "\n",
    "def check_detection_result(img, box):\n",
    "    #bx,by,bw,bh = 307.466186523,196.785614014,86.6007080078,166.478393555\n",
    "    bx,by,bw,bh = box\n",
    "    xmin, ymin, xmax, ymax = int(bx-bw/2), int(by-bh/2), int(bx+bw/2), int(by+bh/2)\n",
    "    return img[ymin:ymax, xmin:xmax]\n",
    "\n",
    "plt.imshow(check_detection_result(frames[16], (370.211120605,174.12197876,88.2487030029,67.7461776733)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frames[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.get(cv2.CAP_PROP_FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/torch_data/UCF-101/ucfTrainTestlist/trainlist01.txt') as f:\n",
    "    ucfList = f.readlines()\n",
    "    labels = [ line.strip().split()[1] for line in ucfList ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Traintests-plit for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_traintestList(root, annotation_path, random_state=0, n_splits=5):\n",
    "    os.system(f'mkdir -p {annotation_path}')\n",
    "\n",
    "    video_dirs = natsort.natsorted(glob.glob(root + '/*/*'))\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(video_dirs)\n",
    "\n",
    "    class2idx = {'adl':0, 'fall':1}\n",
    "\n",
    "    formated_video_dirs = np.array([ x[len(root.rstrip('/'))+1:] for x in video_dirs ])\n",
    "\n",
    "    kf = KFold(n_splits)\n",
    "\n",
    "    for k, (train_ix, test_ix) in enumerate(kf.split(formated_video_dirs)):\n",
    "        _train, _test = np.array(formated_video_dirs)[train_ix], np.array(formated_video_dirs)[test_ix]\n",
    "        _train_lab = [ os.path.dirname(x) for x in _train ]\n",
    "        _test_lab = [ os.path.dirname(x) for x in _test ]\n",
    "        print(f'[splist-{k}] train : {np.unique(_train_lab, return_counts=True)}, test : {np.unique(_test_lab, return_counts=True)}')\n",
    "        for _split, _data in zip(['train', 'test'], [_train, _test]):\n",
    "            lines = []\n",
    "            for i in range(len(_data)):\n",
    "                line = _data[i] + \" \"\n",
    "                if _split == 'train':\n",
    "                    line += str(class2idx[os.path.dirname(_data[i])])\n",
    "\n",
    "                lines.append(line + '\\n')\n",
    "\n",
    "            with open(os.path.join(annotation_path, f\"{_split}list{k+1:02d}.txt\"), 'w') as fp:\n",
    "                fp.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[splist-0] train : (array(['adl', 'fall'], dtype='<U4'), array([29, 49])), test : (array(['adl', 'fall'], dtype='<U4'), array([10, 10]))\n",
      "[splist-1] train : (array(['adl', 'fall'], dtype='<U4'), array([35, 43])), test : (array(['adl', 'fall'], dtype='<U4'), array([ 4, 16]))\n",
      "[splist-2] train : (array(['adl', 'fall'], dtype='<U4'), array([27, 51])), test : (array(['adl', 'fall'], dtype='<U4'), array([12,  8]))\n",
      "[splist-3] train : (array(['adl', 'fall'], dtype='<U4'), array([32, 47])), test : (array(['adl', 'fall'], dtype='<U4'), array([ 7, 12]))\n",
      "[splist-4] train : (array(['adl', 'fall'], dtype='<U4'), array([33, 46])), test : (array(['adl', 'fall'], dtype='<U4'), array([ 6, 13]))\n"
     ]
    }
   ],
   "source": [
    "# URFD\n",
    "create_traintestList(root = '/data/FallDownData/URFD/video/',\n",
    "                     annotation_path = '/data/FallDownData/URFD/TrainTestlist',\n",
    "                     n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[splist-0] train : (array(['adl', 'fall'], dtype='<U4'), array([590, 140])), test : (array(['adl', 'fall'], dtype='<U4'), array([138,  45]))\n",
      "[splist-1] train : (array(['adl', 'fall'], dtype='<U4'), array([578, 152])), test : (array(['adl', 'fall'], dtype='<U4'), array([150,  33]))\n",
      "[splist-2] train : (array(['adl', 'fall'], dtype='<U4'), array([578, 152])), test : (array(['adl', 'fall'], dtype='<U4'), array([150,  33]))\n",
      "[splist-3] train : (array(['adl', 'fall'], dtype='<U4'), array([588, 143])), test : (array(['adl', 'fall'], dtype='<U4'), array([140,  42]))\n",
      "[splist-4] train : (array(['adl', 'fall'], dtype='<U4'), array([578, 153])), test : (array(['adl', 'fall'], dtype='<U4'), array([150,  32]))\n"
     ]
    }
   ],
   "source": [
    "# MulticamFD\n",
    "create_traintestList(root = '/data/FallDownData/MulticamFD/video/',\n",
    "                     annotation_path = '/data/FallDownData/MulticamFD/TrainTestlist',\n",
    "                     n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/hossay/gaitanalysis/preprocess/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/FallDownData/URFD/raw/fall/fall-27-cam1-rgb/fall-27-cam1-rgb-060.png'\n",
    "im = cv2.imread(path,-1)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/torch_data/fall-27-cam1.mp4'\n",
    "cap = cv2.VideoCapture(path)\n",
    "frames = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "    frames.append(frame)\n",
    "\n",
    "im = frames[60][:, 320:]\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "cv2.imwrite('/home/hossay/Downloads/test.png', im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
