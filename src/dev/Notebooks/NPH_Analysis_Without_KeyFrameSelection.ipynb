{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.getcwd())) \n",
    "\n",
    "from utils.generate_model import init_state, load_trained_ckpt\n",
    "from utils.transforms import (\n",
    "    Compose, ToTensor, MultiScaleRandomCrop, MultiScaleCornerCrop, Normalize,\n",
    "    TemporalRandomCrop, TemporalCenterCrop, LoopPadding)\n",
    "from utils.target_columns import get_target_columns\n",
    "from utils.mean import get_mean, get_std\n",
    "from utils.preprocessing import (\n",
    "    PatientLocalizer,\n",
    "    COPAnalyizer,\n",
    "    HumanScaleAnalyizer,\n",
    "    Worker,\n",
    ")\n",
    "import datasets.gaitregression\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn\n",
    "import visdom\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as TF\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "import re\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import collections\n",
    "from dateutil.parser import parse\n",
    "import datetime\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../data/NPH_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add CV columns\n",
    "old_columns = [('Stride Length Std Dev', 'Stride Length(cm)'),\n",
    "               ('Stride Time Std Dev', 'Cycle Time(sec)')]\n",
    "\n",
    "new_columns = ['CV Stride Length', 'CV Stride Time']\n",
    "\n",
    "for new_col, old_cols in zip(new_columns, old_columns):\n",
    "    for tail in ['(LEFT)', '(RIGHT)']:\n",
    "        df[new_col + tail] = 100 * df[old_cols[0] + tail] / df[old_cols[1]+tail]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match time format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_timeformat(x):\n",
    "    if type(x) == datetime.datetime:\n",
    "        return x.strftime(\"%Y-%m-%d\")\n",
    "        #return datetime.datetime(x.year, x.month, x.day)\n",
    "    p = re.match(r'(?P<year>\\d\\d\\d)-(?P<month>\\d\\d)-(?P<day>\\d\\d)', x)\n",
    "    group_map = p.groupdict()\n",
    "    date_groups = [ group_map['month'], group_map['day'], group_map['year']]\n",
    "\n",
    "    for i in range(len(date_groups)):\n",
    "        date_groups[i] = str(int(date_groups[i]))\n",
    "    \n",
    "    dt = parse('/'.join(date_groups))\n",
    "    return dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "df[\"DATE\"] = df[\"DATE\"].map(match_timeformat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a group column to distinguish evaluation groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for pat_id in df[\"PAT ID\"].unique():\n",
    "    patient_df = df[df[\"PAT ID\"] == int(pat_id) ]\n",
    "    #print(f\"Min : {patient_df['DATE'].min()} \\t Max : {patient_df['DATE'].max()}\")    \n",
    "    baseline_group = patient_df[patient_df.DATE==patient_df['DATE'].min()].copy()\n",
    "    baseline_group.loc[:,'GROUP'] = 'baseline'\n",
    "    followup_group = patient_df[patient_df.DATE==patient_df['DATE'].max()].copy()\n",
    "    followup_group.loc[:,'GROUP'] = 'followup'\n",
    "    df_list.append(pd.concat([baseline_group, followup_group], axis=0))\n",
    "    \n",
    "df = pd.concat(df_list, axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config file for demo\n",
    "opt = SimpleNamespace(**json.load(open('../data/demo_config.json')))\n",
    "\n",
    "# attention indicator\n",
    "opt.attention_str = 'Attentive' if opt.attention else 'NonAttentive'\n",
    "opt.group_str = f\"G{opt.n_groups}\" if opt.n_groups > 0 else ''\n",
    "opt.arch = \"{}-{}\".format(opt.backbone, opt.model_depth)\n",
    "\n",
    "target_columns = get_target_columns(opt)\n",
    "\n",
    "opt.mean = get_mean(opt.norm_value, dataset=opt.mean_dataset)\n",
    "opt.std = get_std(opt.norm_value, dataset=opt.mean_dataset)\n",
    "\n",
    "if opt.no_mean_norm and not opt.std_norm:\n",
    "    norm_method = Normalize([0, 0, 0], [1, 1, 1])\n",
    "else:\n",
    "    norm_method = Normalize(opt.mean, opt.std)\n",
    "\n",
    "if opt.train_crop == \"random\":\n",
    "    crop_method = MultiScaleRandomCrop(opt.scales, opt.sample_size)\n",
    "elif opt.train_crop == \"corner\":\n",
    "    crop_method = MultiScaleCornerCrop(opt.scales, opt.sample_size)\n",
    "elif opt.train_crop == \"center\":\n",
    "    crop_method = MultiScaleCornerCrop(\n",
    "        opt.scales, opt.sample_size, crop_positions=[\"c\"]\n",
    "    )\n",
    "\n",
    "spatial_transform = {\n",
    "    \"train\": Compose(\n",
    "        [\n",
    "            TF.RandomRotation(degrees=(0, 0)),\n",
    "            TF.RandomResizedCrop(size=opt.sample_size,\n",
    "                                 scale=(opt.sample_size/opt.img_size, 1.0)),\n",
    "            ToTensor(opt.norm_value),\n",
    "            norm_method,\n",
    "        ]\n",
    "    ),\n",
    "    \"test\": Compose(\n",
    "        [\n",
    "            TF.CenterCrop(opt.sample_size),\n",
    "            ToTensor(opt.norm_value),\n",
    "            norm_method,\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "temporal_transform = {\n",
    "    \"train\": None,  # TemporalRandomCrop(opt.sample_duration),\n",
    "    \"test\": None,  # TemporalCenterCrop(opt.sample_duration),\n",
    "}\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "target_transform = StandardScaler()\n",
    "\n",
    "# prepare dataset\n",
    "_ = datasets.gaitregression.prepare_dataset(\n",
    "    input_file=opt.detection_file,\n",
    "    target_file=opt.target_file,\n",
    "    target_columns=target_columns,\n",
    "    chunk_parts=opt.chunk_parts,\n",
    "    target_transform=target_transform,\n",
    ")\n",
    "\n",
    "# format target columns\n",
    "for old, new in zip(['/L','/R'], ['(LEFT)', '(RIGHT)']):\n",
    "    target_columns = np.char.replace(target_columns, old, new)\n",
    "target_columns = target_columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.benchmark = False\n",
    "\n",
    "# define regression model\n",
    "net, criterion1, criterion2, optimizer, scheduler = init_state(opt)\n",
    "net = load_trained_ckpt(opt, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, dl, target_transform):\n",
    "    res = []\n",
    "    for input_data in tqdm(dl):\n",
    "        out = net(input_data)\n",
    "        reg_outputs = out[0]\n",
    "        reg_outputs = target_transform.inverse_transform(\n",
    "            reg_outputs.detach().cpu().numpy())\n",
    "        res.append(reg_outputs)\n",
    "    \n",
    "    return np.vstack(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "exception_dict = json.load(open('../data/group_exceptions.json'))\n",
    "\n",
    "exception_dict = {\"baseline\": df[df[\"Serial Num\"].isin(exception_dict['baseline'])].FILENAME,\n",
    "              \"followup\": df[df[\"Serial Num\"].isin(exception_dict['followup'])].FILENAME}          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluede samples without any video file\n",
    "patient_list = list(set(df.FILENAME.values)-{\"7181794_basic_test_2_trial_3.txt\", \n",
    "                                             \"7005361_basic_test_4_trial_1.txt\", \n",
    "                                             \"7005361_basic_test_4_trial_0.txt\",\n",
    "                                             \"1909243_basic_test_2_trial_0.txt\",\n",
    "                                             \"1909243_basic_test_2_trial_1.txt\",\n",
    "                                             \"1909243_basic_test_2_trial_2.txt\",\n",
    "                                             \"1909243_basic_test_2_trial_3.txt\",\n",
    "                                             \"1194117_basic_test_1_trial_0.txt\",\n",
    "                                             \"1194117_basic_test_1_trial_1.txt\",\n",
    "                                             \"1194117_basic_test_1_trial_2.txt\",\n",
    "                                             \"1194117_basic_test_1_trial_3.txt\",\n",
    "                                             \"1019153_basic_test_3_trial_0.txt\",\n",
    "                                             \"1019153_basic_test_3_trial_1.txt\",\n",
    "                                             \"1019153_basic_test_3_trial_2.txt\",\n",
    "                                             \"1019153_basic_test_3_trial_3.txt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_NameFormat(x):\n",
    "    x = os.path.splitext(x)[0]\n",
    "    regex = re.compile(r'(.*)_basic_test_(.*)_trial_(.*)')\n",
    "    pid, test_ix, trial_ix = regex.search(x).groups()\n",
    "    return f'{pid}_test_{test_ix}_trial_{trial_ix}' + '.avi'\n",
    "\n",
    "class NPH_Dataset(Dataset):\n",
    "    def __init__(self, opt, data,\n",
    "                 interval_selector, target_columns, spatial_transform):\n",
    "        self.opt = opt\n",
    "    \n",
    "        column_sel = [\"Serial Num\", \"PAT ID\", \"DATE\", \"TIME\", \"FILENAME\", *target_columns]\n",
    "        data = data[column_sel] # only use selected colums\n",
    "        \n",
    "        self.input_files = [ os.path.join(opt.data_root,x) for x in data.FILENAME.map(match_NameFormat).values ]\n",
    "        self.interval_selector = interval_selector\n",
    "        self.spatial_transform = spatial_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video = self.input_files[idx]\n",
    "        vid = os.path.splitext(os.path.basename(video))[0]\n",
    "        \n",
    "        start_ix, end_ix = self.interval_selector.get_interval(\n",
    "            vid=vid, return_index=True)\n",
    "        \n",
    "        if not os.path.exists(video):\n",
    "            print(video)\n",
    "        \n",
    "        cap = cv2.VideoCapture(video)\n",
    "\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # cvt color map (bgr->rgb)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img = Image.fromarray(frame).resize(\n",
    "                (self.opt.img_size, self.opt.img_size))\n",
    "            img_tensor = self.spatial_transform(img)\n",
    "            frames.append(img_tensor)\n",
    "\n",
    "        # time intervals of interest\n",
    "        frames = frames[start_ix:end_ix+1]\n",
    "\n",
    "        frame_indices = torch.LongTensor(\n",
    "            np.linspace(\n",
    "                0, len(frames), self.opt.sample_duration, endpoint=False).astype(np.int)\n",
    "        )\n",
    "        input_data = torch.stack(frames)[frame_indices].permute(1, 0, 2, 3)\n",
    "\n",
    "        padding = (0, 0,\n",
    "                   0, 0,\n",
    "                   0, self.opt.sample_duration - len(frame_indices))\n",
    "\n",
    "        # zero padding\n",
    "        input_data = F.pad(input_data, padding)\n",
    "        return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "def check_file(x, *args, **kwargs):\n",
    "    video = match_NameFormat(x)\n",
    "    vpath = os.path.join(kwargs['opt'].video_home, video)\n",
    "    return os.path.exists(vpath)\n",
    "\n",
    "def get_prediction_result(net, opt, df, interval_selector, condition,\n",
    "                          target_columns, spatial_transform, target_transform):\n",
    "    \n",
    "    df = copy.copy(df)\n",
    "    \n",
    "    # get df of given condition (baseline | followup)\n",
    "    df = df[df.GROUP==condition]\n",
    "    patient_list = list(set(df.FILENAME)-set(exception_dict[condition]))\n",
    "    \n",
    "    pat_s = pd.Series(patient_list)\n",
    "    patient_list = pat_s[pat_s.apply(check_file, opt=opt)].values\n",
    "    \n",
    "    # takes only use existing files\n",
    "    df = df[df.FILENAME.isin(patient_list)]\n",
    "\n",
    "    df['Type'] = np.tile('gt', len(df))\n",
    "    column_sel = [\"Serial Num\", \"PAT ID\", \"DATE\", \"TIME\", \"FILENAME\", *target_columns, \"Type\", \"GROUP\"]\n",
    "    \n",
    "    df = df[column_sel].applymap(lambda x: np.nan if x=='<missing>' else x)\n",
    "\n",
    "    nan_patients = []\n",
    "    for i,x in enumerate((df.isna()).any(axis=1)):\n",
    "        if x:\n",
    "            nan_patients.append(df.iloc[i][\"FILENAME\"])\n",
    "\n",
    "    # remove missing rows\n",
    "    df = df.dropna()\n",
    "\n",
    "    N_samples = len(df)\n",
    "    N_factors = len(target_columns)\n",
    "\n",
    "    print(f'[Evaluation for \\'{condition}\\' group] ### total data samples (n) : {N_samples}, found {len(nan_patients)} missing row(s)...')\n",
    "    \n",
    "    input_data = df.values\n",
    "            \n",
    "    # init dataset\n",
    "    ds = NPH_Dataset(opt, df, interval_selector, target_columns, spatial_transform=spatial_transform)\n",
    "    dl = DataLoader(ds, batch_size=opt.batch_size, shuffle=False, num_workers=8)\n",
    "    \n",
    "    # result from AGNet\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        res = predict(net, dl, target_transform=target_transform)\n",
    "        \n",
    "    res = np.column_stack((df.loc[:,:\"FILENAME\"].values, res, \n",
    "                           np.tile('pred', N_samples), np.tile(condition, N_samples)))\n",
    "    data = np.row_stack((input_data, res))\n",
    "\n",
    "    return pd.DataFrame(data,columns=[\"Serial Num\", \"PAT ID\", \"DATE\", \"TIME\", \"FILENAME\", *target_columns, \"Type\", \"GROUP\"])\n",
    "\n",
    "res = {}\n",
    "\n",
    "for condition in ['baseline', 'followup']:\n",
    "    res[condition] = get_prediction_result(net, \n",
    "                                           opt, \n",
    "                                           df, \n",
    "                                           interval_selector = COPAnalyizer(opt.meta_home, opt.fps),\n",
    "                                           condition=condition,\n",
    "                                           target_columns=target_columns,\n",
    "                                           spatial_transform=spatial_transform['test'],\n",
    "                                           target_transform=target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplots(target_columns, y_true, y_pred, scores, condition):\n",
    "    pred_and_gt = {k: [] for k in target_columns}\n",
    "\n",
    "    for i, col in enumerate(target_columns):\n",
    "        pred_and_gt[col].append([y_pred[:, i], y_true[:, i]])\n",
    "\n",
    "    data = collections.defaultdict(list)\n",
    "\n",
    "    pp = []\n",
    "    gg = []\n",
    "    for i, col in enumerate(pred_and_gt.keys()):\n",
    "        transposed_data = list(zip(*pred_and_gt[col]))\n",
    "        preds = np.concatenate(transposed_data[0])\n",
    "        gts = np.concatenate(transposed_data[1])\n",
    "\n",
    "        pp.append(preds)\n",
    "        gg.append(gts)\n",
    "\n",
    "        for p, g in zip(preds, gts):\n",
    "            data[\"name\"].append(col)\n",
    "            data[\"pred\"].append(p)\n",
    "            data[\"gt\"].append(g)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=5, ncols=4, figsize=(20, 20))\n",
    "    fig.suptitle(fr'$ScatterPlots$ ({condition})', fontsize=30, y=0.98)\n",
    "\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (preds, gts) in enumerate(zip(pp, gg)):\n",
    "        ax = axes[i]\n",
    "        preds = np.array(preds)\n",
    "        gts = np.array(gts)\n",
    "\n",
    "        ax.set_xlim(min(gts), max(gts))\n",
    "        ax.set_ylim(min(preds), max(preds))\n",
    "        \n",
    "        ax.scatter(gts, preds, c='g', label='data')\n",
    "        ax.plot([min(gts), max(gts)], [min(gts), max(gts)],\n",
    "                'r--', label='GT-line')\n",
    "        ax.set_xlabel('GT', fontsize=15)\n",
    "        ax.set_ylabel('PRED', fontsize=15)\n",
    "        \n",
    "        ax.set_title(f'name={target_columns[i]}\\n' + r'[$r^2 = {:0.3f}$]'.format(scores[i]), fontdict={'fontsize': 15})\n",
    "        ax.legend(loc='lower right')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(hspace=0.4, top=0.9)\n",
    "\n",
    "    \n",
    "    \n",
    "def BlandAltmanplots(target_columns, y_true, y_pred, condition):\n",
    "    pred_and_gt = {k: [] for k in target_columns}\n",
    "\n",
    "    for i, col in enumerate(target_columns):\n",
    "        pred_and_gt[col].append([y_pred[:, i], y_true[:, i]])\n",
    "\n",
    "    data = collections.defaultdict(list)\n",
    "\n",
    "    pp = []\n",
    "    gg = []\n",
    "    for i, col in enumerate(pred_and_gt.keys()):\n",
    "        transposed_data = list(zip(*pred_and_gt[col]))\n",
    "        preds = np.concatenate(transposed_data[0])\n",
    "        gts = np.concatenate(transposed_data[1])\n",
    "\n",
    "        pp.append(preds)\n",
    "        gg.append(gts)\n",
    "\n",
    "        for p, g in zip(preds, gts):\n",
    "            data[\"name\"].append(col)\n",
    "            data[\"pred\"].append(p)\n",
    "            data[\"gt\"].append(g)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    fig.suptitle(fr'$BlandAltmanPlots$ ({condition})', fontsize=30, y=0.98)\n",
    "\n",
    "    for i, (preds, gts) in enumerate(zip(pp, gg)):\n",
    "        ax = fig.add_subplot(5,4,i+1)\n",
    "        preds = np.array(preds)\n",
    "        gts = np.array(gts)\n",
    "        sm.graphics.mean_diff_plot(preds, gts, ax = ax)\n",
    "        ax.set_title(f'name={target_columns[i]}\\n' + r'[$r^2 = {:0.3f}$]'.format(scores[i]), fontdict={'fontsize': 15})\n",
    "        \n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(hspace=0.4, top=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.regression import r2_score\n",
    "\n",
    "analysis_data = {}\n",
    "for condition in ['baseline', 'followup']:\n",
    "    result = res[condition]\n",
    "\n",
    "    # y_true / y_pred\n",
    "    y_true = result[result.Type=='gt'].loc[:,target_columns[0]:target_columns[-1]]\n",
    "    y_pred = result[result.Type=='pred'].loc[:,target_columns[0]:target_columns[-1]]\n",
    "    \n",
    "    scores = r2_score(y_true, y_pred, multioutput='raw_values')\n",
    "\n",
    "    analysis_data[condition] = y_true, y_pred, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# scatter plots\n",
    "for condition in analysis_data:\n",
    "    y_true, y_pred, scores = analysis_data.get(condition)\n",
    "    scatterplots(target_columns, y_true.values, y_pred.values, scores, condition)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bland-Altman Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# BlandAltmanplots \n",
    "for condition in analysis_data:\n",
    "    y_true, y_pred, scores = analysis_data.get(condition)\n",
    "    BlandAltmanplots(target_columns[:-4], y_true.values, y_pred.values, condition)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson's coefficents(GT<->PRED) of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "def compute_pearson_coeff(x,y):\n",
    "    if any(x.columns != y.columns):\n",
    "        raise Exception('columns of x and y are not matched!')\n",
    "    \n",
    "    columns = x.columns\n",
    "    \n",
    "    res = []\n",
    "    for col in columns:\n",
    "        r_val, p_val = pearsonr(x[col], y[col])\n",
    "        res.append((col,r_val,p_val))\n",
    "        \n",
    "    return pd.DataFrame(res, columns=['PARAMS','r','p-val'])\n",
    "\n",
    "for condition in analysis_data:\n",
    "    y_true, y_pred, scores = analysis_data.get(condition)\n",
    "    print(f'[Pearson\\'s coeff (R) for \\'{condition}\\' group @ N={len(y_true)}]\\n\\n', compute_pearson_coeff(y_true, y_pred), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data = res['baseline']\n",
    "baseline_data = {'gt': baseline_data[baseline_data.Type=='gt'],\n",
    "                 'pred': baseline_data[baseline_data.Type=='pred']}\n",
    "\n",
    "followup_data = res['followup']\n",
    "followup_data = {'gt': followup_data[followup_data.Type=='gt'],\n",
    "                 'pred': followup_data[followup_data.Type=='pred']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_data_gt = pd.concat((baseline_data['gt'], followup_data['gt']))\n",
    "grouped_data_gt = all_data_gt.groupby([\"PAT ID\", \"GROUP\"]).apply(lambda x: x.mean()).loc[:, target_columns[0]:target_columns[-5]]\n",
    "\n",
    "delta_gt = (grouped_data_gt.xs('followup', level=\"GROUP\")-grouped_data_gt.xs('baseline', level=\"GROUP\"))/grouped_data_gt.xs('baseline', level=\"GROUP\")\n",
    "delta_gt = delta_gt.dropna() # remove all un-paired rows\n",
    "delta_gt *= 100\n",
    "\n",
    "grouped_data_gt.columns = [ x+'_gt' for x in grouped_data_gt.columns ]\n",
    "delta_gt.columns = [ r'$\\Delta$'+x+'_gt' for x in delta_gt.columns ]\n",
    "\n",
    "all_data_pred = pd.concat((baseline_data['pred'], followup_data['pred']))\n",
    "grouped_data_pred = all_data_pred.groupby([\"PAT ID\", \"GROUP\"]).apply(lambda x: x.mean()).loc[:, target_columns[0]:target_columns[-5]]\n",
    "\n",
    "delta_pred =(grouped_data_pred.xs('followup', level=\"GROUP\")-grouped_data_pred.xs('baseline', level=\"GROUP\"))/grouped_data_pred.xs('baseline', level=\"GROUP\")\n",
    "delta_pred = delta_pred.dropna() # remove all un-paired rows\n",
    "delta_pred *= 100\n",
    "\n",
    "grouped_data_pred.columns = [ x+'_pred' for x in grouped_data_pred.columns ]\n",
    "delta_pred.columns = [ r'$\\Delta$'+x+'_pred' for x in delta_pred.columns ]\n",
    "\n",
    "# integrated df\n",
    "integrated_df = pd.concat((grouped_data_gt, grouped_data_pred), axis=1)\n",
    "integrated_df.reset_index(level=[\"GROUP\"], inplace=True)\n",
    "\n",
    "# integrated delta_dfs\n",
    "integrated_delta_df = pd.concat((delta_gt, delta_pred), axis=1)\n",
    "\n",
    "print(f'# of fully paired patients (i.e. both baseline & followup exists!) : {len(integrated_delta_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data_pred.xs('followup', level=\"GROUP\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trends in improvement ($\\Delta\\$) % for each gait param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name in target_columns[:-4]:\n",
    "    g = sns.pairplot(integrated_delta_df, diag_kind='kde',\n",
    "             vars=[r'$\\Delta$'+f'{name}_gt',\n",
    "                   r'$\\Delta$'+f'{name}_pred'], kind='reg')\n",
    "    g.fig.subplots_adjust(top=0.9)\n",
    "    g.fig.suptitle(rf'Trends of $\\Delta$({name})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trends in each param for {baseline/followup} groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name in target_columns[:-4]:\n",
    "    g = sns.pairplot(integrated_df, vars=[f'{name}_gt', \n",
    "                                      f'{name}_pred'], hue=\"GROUP\")\n",
    "    g.fig.subplots_adjust(top=0.9)\n",
    "    g.fig.suptitle(f'Trends of {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trends in improvement per each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaged_improvement(data):\n",
    "    # compute improvements for patients with followup data\n",
    "    patients = np.unique(data[data.GROUP=='followup'][\"PAT ID\"])\n",
    "    \n",
    "    res = []\n",
    "\n",
    "    for p in patients:\n",
    "        p_data = data[data[\"PAT ID\"]==p]\n",
    "        baseline_vals = p_data[p_data[\"GROUP\"]=='baseline']\n",
    "        follow_vals = p_data[p_data[\"GROUP\"]=='followup']\n",
    "        improvements = (follow_vals.mean()-baseline_vals.mean())[target_columns[:-4]].values\n",
    "        res.append(improvements)\n",
    "    \n",
    "    return np.vstack(res)\n",
    "\n",
    "gt_imprmt = averaged_improvement(data=all_data_gt)\n",
    "pred_imprmt = averaged_improvement(data=all_data_pred)\n",
    "\n",
    "# accuracy of predicting improvements trend\n",
    "print(f\"Accuracy of predicting improvements trend : \\n {np.mean(np.sign(gt_imprmt)==np.sign(pred_imprmt), axis=1).tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_improvement_plot(data, column_name=\"Cadence\"):\n",
    "    # compute improvements for patients with followup data\n",
    "    patients = np.unique(data[data.GROUP=='followup'][\"PAT ID\"])\n",
    "\n",
    "    res = defaultdict(list)\n",
    "\n",
    "    for p in patients:\n",
    "        p_data = data[data[\"PAT ID\"]==p]\n",
    "        baseline_vals = p_data[p_data[\"GROUP\"]=='baseline'].mean()[target_columns[:-4]]\n",
    "        follow_vals = p_data[p_data[\"GROUP\"]=='followup'].mean()[target_columns[:-4]]\n",
    "\n",
    "        for k, v in zip(['baseline', 'followup'], [baseline_vals, follow_vals]):\n",
    "            res[k].append(v.values)\n",
    "\n",
    "    for k in res.keys():\n",
    "        res[k] = np.array(res[k])\n",
    "\n",
    "    plt.figure(figsize=(7,10))\n",
    "    plt.xticks([0,1], ['baseline', 'followup'])\n",
    "\n",
    "    c_ix = target_columns[:-4].index(column_name)\n",
    "\n",
    "    plt.scatter([0]*len(patients), res['baseline'][:,c_ix], marker='s', c='k')\n",
    "    plt.scatter([1]*len(patients), res['followup'][:,c_ix], marker='s', c='k')\n",
    "    for i in range(len(patients)):\n",
    "        plt.plot([0,1], [res['baseline'][i,c_ix], res['followup'][i,c_ix]], 'g-')\n",
    "\n",
    "    plt.plot([-0.3], [res['baseline'][:,c_ix].mean()], 'ro', markersize=10)\n",
    "    plt.plot([1.3], [res['followup'][:,c_ix].mean()], 'ro', markersize=10)\n",
    "    plt.plot([-0.3, 1.3], [res['baseline'][:,c_ix].mean(), res['followup'][:,c_ix].mean()], 'r--')\n",
    "    \n",
    "    plt.plot([-0.3]*2, [res['baseline'][:,c_ix].mean()+res['baseline'][:,c_ix].std(),\n",
    "                       res['baseline'][:,c_ix].mean()-res['baseline'][:,c_ix].std()], 'r-')\n",
    "    plt.plot([1.3]*2, [res['followup'][:,c_ix].mean()+res['followup'][:,c_ix].std(),\n",
    "                       res['followup'][:,c_ix].mean()-res['followup'][:,c_ix].std()], 'r-')\n",
    "    \n",
    "    mean_slope = (res['followup'][:, c_ix].mean()-res['baseline'][:, c_ix].mean())/1.6\n",
    "    plt.text(1.3+0.05, res['followup'][:, c_ix].mean(), f'mean_slope : {mean_slope:.3f}')\n",
    "    \n",
    "    plt.title(f'[{data.iloc[0].Type}] Scatters of {column_name} per each subjects')\n",
    "\n",
    "    plt.xlim(-0.5, 2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "paired_improvement_plot(data=all_data_gt)\n",
    "paired_improvement_plot(data=all_data_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatters_per_each_subject(data, column_name):\n",
    "    ids = np.unique(data[\"PAT ID\"])\n",
    "    id2ix = dict(zip(ids, range(len(ids))))\n",
    "    data[\"PAT ID\"] = data[\"PAT ID\"].map(id2ix)\n",
    "\n",
    "    for column in target_columns[:-4]:\n",
    "        data[column] = data[column].astype(float)\n",
    "\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    sns.scatterplot(x=\"PAT ID\", y=column_name, hue='GROUP', style='GROUP', data=data)\n",
    "    plt.title(f'[{data.iloc[0].Type}] Scatters of {column_name} per each subjects')\n",
    "    \n",
    "scatters_per_each_subject(data=all_data_gt, column_name='Cadence')\n",
    "scatters_per_each_subject(data=all_data_pred, column_name='Cadence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distributions_method_effect(pat_id):\n",
    "    pat_data_gt = all_data_gt[all_data_gt[\"PAT ID\"]==pat_id]\n",
    "    pat_data_pred = all_data_pred[all_data_pred[\"PAT ID\"]==pat_id]\n",
    "\n",
    "    def modify_columns(d, suf):\n",
    "        c = []\n",
    "        for x in d.columns:\n",
    "            suffix = ''\n",
    "            if x in target_columns:\n",
    "                suffix = suf\n",
    "            c.append(x+suffix)\n",
    "\n",
    "        d.columns = c\n",
    "\n",
    "    modify_columns(pat_data_gt, suf='_gt')\n",
    "    modify_columns(pat_data_pred, suf='_pred')\n",
    "\n",
    "    pat_data = pd.merge(pat_data_gt, pat_data_pred, on='Serial Num')\n",
    "    \n",
    "    for name in target_columns[:-4]:\n",
    "        g = sns.pairplot(pat_data, vars=[f'{name}_gt', \n",
    "                                          f'{name}_pred'], hue=\"GROUP_x\")\n",
    "        g.fig.subplots_adjust(top=0.9)\n",
    "        g.fig.suptitle(f'Trends of {name}')\n",
    "\n",
    "plot_distributions_method_effect(pat_id = 1789841)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix of improvements ($\\Delta\\$) for each param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = integrated_delta_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix (full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = integrated_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count summaries\n",
    "print(\"#\"*8, \"COUNT SUMMARIES\", \"#\"*8)\n",
    "print(f\"Patients used in \\'baseline\\' exp {np.unique(integrated_df[integrated_df.GROUP=='baseline'].index).shape[0]}\")\n",
    "print(f\"Patients used in \\'followup\\' exp {np.unique(integrated_df[integrated_df.GROUP=='followup'].index).shape[0]}\")\n",
    "print(f\"Patients used in \\'delta\\' exp {len(integrated_delta_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_df[integrated_df.GROUP=='baseline'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_df[integrated_df.GROUP=='followup'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['followup'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map = {'all-gt': all_data_gt,\n",
    "          'baseline-gt': res['baseline'][res['baseline'].Type=='gt'],\n",
    "          'baseline-pred': res['baseline'][res['baseline'].Type=='pred'],\n",
    "          'followup-gt': res['followup'][res['followup'].Type=='gt'],\n",
    "          'followup-pred': res['followup'][res['followup'].Type=='pred'],\n",
    "          'delta-gt': delta_gt,\n",
    "          'delta-pred': delta_pred}\n",
    "\n",
    "# output excel file\n",
    "writer = pd.ExcelWriter('../data/NPH_analysis_results.xlsx', engine='xlsxwriter')\n",
    "with writer:\n",
    "    worksheets = []\n",
    "\n",
    "    for title in df_map:\n",
    "        worksheets.append(\n",
    "            ( title, writer.book.add_worksheet(title) )\n",
    "        )\n",
    "\n",
    "    # copy existing sheets\n",
    "    writer.sheets = dict((title, ws) for title,ws in worksheets)\n",
    "\n",
    "    for title in writer.sheets:\n",
    "        # Convert the dataframe to an XlsxWriter Excel object.\n",
    "        df_map.get(title).to_excel(writer, sheet_name=title, index=(title.startswith('delta')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 PARAMS         r          p-val\n",
      "0              Velocity  0.969343   1.142070e-78\n",
      "1               Cadence  0.858185   2.674705e-38\n",
      "2       Cycle Time(sec)  0.856396   5.555690e-38\n",
      "3     Stride Length(cm)  0.985972  7.755956e-100\n",
      "4   HH Base Support(cm)  0.952481   6.623812e-67\n",
      "5      Swing % of Cycle  0.894479   7.092205e-46\n",
      "6     Stance % of Cycle  0.894348   7.633484e-46\n",
      "7   Double Supp % Cycle  0.890450   6.599517e-45\n",
      "8          Toe In / Out  0.743212   9.669139e-24\n",
      "9      CV Stride Length  0.782745   1.006557e-27\n",
      "10       CV Stride Time  0.812695   2.423049e-31\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "def compute_pearson_coeff(x,y, out_cols=['PARAMS','r','p-val']):\n",
    "    if any(x.columns != y.columns):\n",
    "        raise Exception('columns of x and y are not matched!')\n",
    "    \n",
    "    columns = x.columns\n",
    "    \n",
    "    res = []\n",
    "    for col in columns:\n",
    "        r_val, p_val = pearsonr(x[col], y[col])\n",
    "        res.append((col,r_val,p_val))\n",
    "        \n",
    "    return pd.DataFrame(res, columns=out_cols)\n",
    "\n",
    "def arange_df(input_data, target_columns):\n",
    "    _left, _right = input_data[target_columns[2::2]], input_data[target_columns[3::2]]\n",
    "    _data = np.column_stack((input_data[target_columns[:2]].values, ((_left.values + _right.values)/2)))\n",
    "    \n",
    "    new_columns = target_columns[:2] + [ x.replace('(LEFT)', '') for x in _left.columns ]\n",
    "\n",
    "    _df = pd.DataFrame(_data, columns=new_columns)\n",
    "    \n",
    "    return _df\n",
    "\n",
    "y_true = pd.read_excel('../data/NPH_analysis_results.xlsx', 3)[target_columns]\n",
    "y_pred = pd.read_excel('../data/NPH_analysis_results.xlsx', 4)[target_columns]\n",
    "\n",
    "# arange data\n",
    "y_true = arange_df(y_true, target_columns)\n",
    "y_pred = arange_df(y_pred, target_columns)\n",
    "\n",
    "print(compute_pearson_coeff(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaged_improvement(data, target_columns):\n",
    "    # compute improvements for patients with followup data\n",
    "    patients = np.unique(data[data.GROUP=='followup'][\"PAT ID\"])\n",
    "    \n",
    "    res = []\n",
    "\n",
    "    for p in patients:\n",
    "        p_data = data[data[\"PAT ID\"]==p]\n",
    "        baseline_vals = p_data[p_data[\"GROUP\"]=='baseline']\n",
    "        follow_vals = p_data[p_data[\"GROUP\"]=='followup']\n",
    "        improvements = (follow_vals.mean()-baseline_vals.mean())[target_columns].values\n",
    "        res.append(improvements)\n",
    "    \n",
    "    return pd.DataFrame(np.vstack(res), columns=target_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "followup_gt = pd.read_excel('../data/NPH_analysis_results.xlsx', sheet_name='followup-gt')\n",
    "followup_pred = pd.read_excel('../data/NPH_analysis_results.xlsx', sheet_name='followup-pred')\n",
    "baseline_gt = pd.read_excel('../data/NPH_analysis_results.xlsx', sheet_name='baseline-gt')\n",
    "baseline_pred = pd.read_excel('../data/NPH_analysis_results.xlsx', sheet_name='baseline-gt')\n",
    "\n",
    "all_data_gt = pd.concat((followup_gt, baseline_gt))\n",
    "all_data_pred = pd.concat((followup_pred, baseline_pred))\n",
    "\n",
    "gt_imprvmt = averaged_improvement(data=all_data_gt, target_columns=target_columns)[target_columns]\n",
    "pred_imprvmt = averaged_improvement(data=all_data_pred, target_columns=target_columns)[target_columns]\n",
    "\n",
    "gt_imprvmt = arange_df(gt_imprvmt, target_columns)\n",
    "pred_imprvmt = arange_df(pred_imprvmt, target_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    (FOLLOWUP-BASELINE)         r         p-val\n",
      "0              Velocity  0.934668  5.165665e-15\n",
      "1               Cadence  0.808483  2.177634e-08\n",
      "2       Cycle Time(sec)  0.850075  7.433462e-10\n",
      "3     Stride Length(cm)  0.957764  8.688328e-18\n",
      "4   HH Base Support(cm)  0.866746  1.425444e-10\n",
      "5      Swing % of Cycle  0.739118  1.355132e-06\n",
      "6     Stance % of Cycle  0.741428  1.206376e-06\n",
      "7   Double Supp % Cycle  0.753365  6.483801e-07\n",
      "8          Toe In / Out  0.324759  6.974948e-02\n",
      "9      CV Stride Length  0.633608  9.907397e-05\n",
      "10       CV Stride Time  0.889923  9.527934e-12\n"
     ]
    }
   ],
   "source": [
    "print(compute_pearson_coeff(gt_imprvmt, pred_imprvmt, out_cols=[\"(FOLLOWUP-BASELINE)\", \"r\", \"p-val\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D object at 0x7ff34cc803c8>]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(gt_imprvmt[\"Double Supp % Cycle\"].values,pred_imprvmt[\"Double Supp % Cycle\"].values, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
