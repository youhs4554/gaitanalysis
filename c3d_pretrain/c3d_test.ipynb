{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from itertools import chain\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from IPython.core.debugger import set_trace\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import cv2\n",
    "from natsort import natsorted\n",
    "import collections\n",
    "from IPython import display\n",
    "import pylab as pl\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics.regression import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.helper import predefined_split\n",
    "from sklearn.metrics.regression import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\n",
    "from skorch import callbacks\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import c3d_wrapper\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "MODEL_PATH = '/data/GaitData/pretrained/C3D/conv3d_deepnetA_sport1m_iter_1900000_TF.model'\n",
    "MEAN_FILE = 'train01_16_128_171_mean.npy'\n",
    "FRAME_HOME = \"/data/GaitData/CroppedFrameArrays\"\n",
    "FRAMES_PER_CLIP = 16\n",
    "FRAME_MAXLEN=300\n",
    "FEATS_MAXLEN=20\n",
    "target_columns = pd.read_pickle(\"../preprocess/data/targets_dataframe.pkl\").columns.values[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_Model:\n",
    "    def __init__(self, batch_size=BATCH_SIZE, model_path=MODEL_PATH, mean_file=MEAN_FILE):\n",
    "        # define graph\n",
    "        net = c3d_wrapper.C3DNet(\n",
    "            pretrained_model_path=model_path, trainable=False,\n",
    "            batch_size=batch_size)\n",
    "\n",
    "        self.tf_video_clip = tf.placeholder(tf.float32,\n",
    "                                       [batch_size, None, 112, 112, 3],\n",
    "                                       name='tf_video_clip')  # (batch,num_frames,112,112,3)\n",
    "        self.tf_output = net(inputs=self.tf_video_clip)\n",
    "        \n",
    "        self.mean_val = np.load(mean_file).transpose(1,2,3,0)\n",
    "\n",
    "            \n",
    "        # create session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        \n",
    "        self.sess = tf.Session(config=config)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def run(self, X):\n",
    "        return self.sess.run(self.tf_output, feed_dict={self.tf_video_clip: [X]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize with pretrained weight file...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = TF_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pid2vid(pid):\n",
    "    num, test_id, trial_id = pid.split('_')\n",
    "    return '_'.join([num, 'test', test_id, 'trial', trial_id])\n",
    "    \n",
    "\n",
    "def vid2pid(vid):\n",
    "    split = vid.split('_')\n",
    "    return '_'.join([split[0], split[2], split[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAITDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 X, y, scaler, feature_extraction_model,\n",
    "                 frame_home=FRAME_HOME, frames_per_clip=FRAMES_PER_CLIP, \n",
    "                 frame_maxlen=FRAME_MAXLEN, feats_maxlen=FEATS_MAXLEN):\n",
    "        \n",
    "        self.frame_home = frame_home\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.vids = [ pid2vid(pid) for pid in self.y.index ]\n",
    "        \n",
    "        self.feature_extraction_model = feature_extraction_model  # tf model\n",
    "        \n",
    "        self.frames_per_clip = frames_per_clip\n",
    "        self.frame_maxlen = frame_maxlen\n",
    "        \n",
    "        # frame_maxlen=300 -> maxlen/frames_per_clip = 18.75 => set feats_maxlen as 20!\n",
    "        self.feats_maxlen = feats_maxlen\n",
    "        \n",
    "        \n",
    "        if scaler:\n",
    "            scaled_values = scaler.transform(y)\n",
    "            self.y.loc[:,:] = scaled_values\n",
    "            \n",
    "    \n",
    "        \n",
    "    def extract_features(self, stacked_arr):\n",
    "\n",
    "        def preprocess_clip(clip):\n",
    "            vid = []\n",
    "            for img in clip:\n",
    "                vid.append(cv2.resize(img, (171,128)))\n",
    "            \n",
    "            vid = np.array(vid)\n",
    "            \n",
    "            leng = len(vid)\n",
    "            \n",
    "            vid = vid - self.feature_extraction_model.mean_val[:leng]\n",
    "            vid = vid[:, 8:120, 30:142, :]\n",
    "\n",
    "            return vid\n",
    "\n",
    "        res = []\n",
    "                \n",
    "        while True:\n",
    "            clip = stacked_arr[:self.frames_per_clip]\n",
    "            if len(clip) == 0: break\n",
    "            \n",
    "            clip = preprocess_clip(clip)\n",
    "            \n",
    "            \n",
    "            # (D, H, W, C) -> (C, D, H, W)\n",
    "            feature = self.feature_extraction_model.run(clip)[0].transpose(3,0,1,2)\n",
    "            res.append(feature)\n",
    "            \n",
    "            # move to next slice !\n",
    "            stacked_arr = stacked_arr[self.frames_per_clip:]\n",
    "        \n",
    "        res = np.concatenate(res, axis=1)\n",
    "        \n",
    "        # zero padding for feature sequence\n",
    "        res = np.pad(res, ((0,0),(0,self.feats_maxlen-res.shape[1]),(0,0),(0,0)), 'constant')\n",
    "        \n",
    "        return res\n",
    "\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.vids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        vid = self.vids[idx]\n",
    "        positions = [ eval(val) for val in self.X.loc[self.X.vids==vid].pos.values ]\n",
    "        \n",
    "        stacked_arr = np.load(os.path.join(self.frame_home, vid) + '.npy')\n",
    "        \n",
    "        feats = self.extract_features(stacked_arr)\n",
    "        \n",
    "        frames = []        \n",
    "\n",
    "        for cropped in stacked_arr:  \n",
    "            pic = cv2.resize(cv2.cvtColor(cropped, cv2.COLOR_BGR2GRAY), (64,64))[:,:,None]\n",
    "            \n",
    "            pic = TF.to_tensor(pic) # scale to [0.0, 1.0]\n",
    "            pic = TF.normalize(pic, (0.5,), (0.5,)).permute(1,2,0).numpy()   # scale to [-1.0, 1.0]\n",
    "            frames.append(pic)\n",
    "            \n",
    "        targets = self.y.loc[vid2pid(vid)].values\n",
    "\n",
    "        # zero padding\n",
    "        frames = np.pad(frames, ((0,self.frame_maxlen-len(frames)),(0,0),(0,0),(0,0)),\n",
    "                                               'constant', constant_values=0).transpose(3,0,1,2)\n",
    "        \n",
    "        return torch.tensor(feats, dtype=torch.float32), torch.tensor(frames, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_filters = [512,256,128,64,1]):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.decode = nn.Sequential(\n",
    "            # b, 256, 39, 8, 8\n",
    "            nn.ConvTranspose3d(num_filters[0], num_filters[1], \n",
    "                               kernel_size=(3,4,4), stride=2, \n",
    "                               padding=1),\n",
    "            nn.BatchNorm3d(num_filters[1]), \n",
    "            nn.ReLU(True),\n",
    "                        \n",
    "            # b, 128, 75, 16, 16\n",
    "            nn.ConvTranspose3d(num_filters[1], num_filters[2], \n",
    "                               kernel_size=(3,4,4), stride=2,\n",
    "                               padding=(2,1,1)),\n",
    "            nn.BatchNorm3d(num_filters[2]), \n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # b, 32, 150, 32, 32\n",
    "            nn.ConvTranspose3d(num_filters[2], num_filters[3], kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(num_filters[3]), \n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # b, 1, 300, 64, 64\n",
    "            nn.ConvTranspose3d(num_filters[3], num_filters[4], kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            x : size = (B, C, D, H, W)\n",
    "        '''\n",
    "        return self.decode(x)\n",
    "    \n",
    "    \n",
    "class Reconstructor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Reconstructor, self).__init__()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "            \n",
    "    def forward(self, encoded):\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def filter_input_df_with_vids(df, vids):\n",
    "    return df[df['vids'].isin(vids)]\n",
    "\n",
    "def filter_target_df_with_vids(df, vids):\n",
    "    target_ids = [ vid2pid(vid) for vid in vids ]\n",
    "    return df.loc[target_ids]\n",
    "\n",
    "def split_dataset_with_vids(input_df, target_df, vids, test_size=0.3, random_state=42):\n",
    "    train_vids, test_vids = train_test_split(vids, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    train_X, train_y = filter_input_df_with_vids(input_df,train_vids), filter_target_df_with_vids(target_df,train_vids)\n",
    "    test_X, test_y = filter_input_df_with_vids(input_df,test_vids), filter_target_df_with_vids(target_df, test_vids)\n",
    "        \n",
    "    return train_X, train_y, train_vids, test_X, test_y, test_vids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import Callback\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = np.clip(x, 0.0, 1.0)\n",
    "    x = 255*x\n",
    "    return x.astype(np.uint8)\n",
    "\n",
    "def to_tensor_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 64, 64)\n",
    "    return x\n",
    "\n",
    "class SaveResults(Callback):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        for name in ['train', 'valid']:\n",
    "            dataset = kwargs['dataset_'+name]\n",
    "            rand_ix = np.random.randint(len(dataset))\n",
    "            X,y = dataset[rand_ix]\n",
    "            \n",
    "            save_dir = os.path.join(self.path, name)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            \n",
    "            # target img\n",
    "            y = y.numpy().transpose(1,2,3,0)  # (maxlen,h,w,3)\n",
    "            \n",
    "            # predicted img\n",
    "            pred = net.predict(X[None,:])[0].transpose(1,2,3,0) # (maxlen,h,w,3)\n",
    "            \n",
    "            for sub_name,pic in zip(['target', 'pred'], [y,pred]):\n",
    "                pic = to_tensor_img(torch.from_numpy(pic))\n",
    "                save_image(pic, os.path.join(save_dir,sub_name+'.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238d9bbcbe5948a7a3d8c4b63a8fa0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=63), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset path\n",
    "input_file = \"../preprocess/data/person_detection_and_tracking_results_drop.pkl\"\n",
    "target_file = \"../preprocess/data/targets_dataframe.pkl\"\n",
    "\n",
    "input_df = pd.read_pickle(input_file)\n",
    "target_df = pd.read_pickle(target_file)[target_columns]\n",
    "\n",
    "possible_vids = list(set(input_df.vids))\n",
    "train_X, train_y, train_vids, test_X, test_y, test_vids = split_dataset_with_vids(input_df, target_df, possible_vids, test_size=0.3, random_state=42)\n",
    "\n",
    "# # target scaler\n",
    "# scaler = StandardScaler()\n",
    "# train_y.loc[:,:] = scaler.fit_transform(train_y.values)\n",
    "\n",
    "scaler = None\n",
    "\n",
    "# holdouf test set for final evaluation\n",
    "test_dataset = GAITDataset(test_X, test_y, scaler, feature_extraction_model=model)\n",
    "test_batcher = DataLoader(test_dataset,batch_size=10, shuffle=False, num_workers=16)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "train_vids = np.array(train_vids)\n",
    "\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "class MyCriterion(_Loss):\n",
    "    def __init__(self):\n",
    "        super(MyCriterion, self).__init__()\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        valid_mask = ~(y.view(y.size(0),FRAME_MAXLEN,-1)==0).all(dim=2)\n",
    "        valid_mask = valid_mask.float()\n",
    "        return torch.mean(torch.sum((valid_mask * ((x-y)**2).mean((1,3,4))),1)/torch.sum(valid_mask,1))\n",
    "\n",
    "# cross validation loop\n",
    "scores = {'MAPE': [], 'MAE': [], 'RMSE': [], 'R2': [], 'Explained variation': []}\n",
    "\n",
    "for train, valid in kf.split(train_vids):\n",
    "    # split trainset with train/valid\n",
    "    train_split, valid_split = train_vids[train], train_vids[valid]\n",
    "    \n",
    "    train_X, train_y = filter_input_df_with_vids(input_df,train_split), filter_target_df_with_vids(target_df,train_split)\n",
    "    valid_X, valid_y = filter_input_df_with_vids(input_df,valid_split), filter_target_df_with_vids(target_df,valid_split)\n",
    "\n",
    "\n",
    "    # dsataset !!\n",
    "    train_dataset = GAITDataset(train_X, train_y, scaler, feature_extraction_model=model)\n",
    "    valid_dataset = GAITDataset(valid_X, valid_y, scaler, feature_extraction_model=model)\n",
    "    \n",
    "    # Init net !\n",
    "    net = NeuralNetRegressor(\n",
    "        Reconstructor,\n",
    "        batch_size=32,\n",
    "        max_epochs=100,\n",
    "        lr=1e-3,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        optimizer__weight_decay=1e-5,\n",
    "        #optimizer__momentum=0.9,\n",
    "        #optimizer__nesterov=True,\n",
    "        criterion=MyCriterion,\n",
    "        device='cuda',\n",
    "        train_split=predefined_split(valid_dataset),\n",
    "        # Shuffle training data on each epoch\n",
    "        iterator_train__shuffle=True,\n",
    "        callbacks=[#('ealy_stop', callbacks.EarlyStopping()),\n",
    "                   #('lr_scheduler', callbacks.LRScheduler(policy='WarmRestartLR', base_period=2)),\n",
    "                   ('prog_bar', callbacks.ProgressBar()),\n",
    "                   ('save_results', SaveResults(path='./results'))\n",
    "                   ],\n",
    "    \n",
    "    )\n",
    "    \n",
    "    net.fit(train_dataset, y=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
