{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp1. Activation-map visualization (AGNet vs. GuidelessNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"9\"\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "import yaml\n",
    "import utils.data\n",
    "from utils.generate_model import load_trained_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(attention=False, backbone='r2plus1d_18', batch_size=64, ckpt_dir='/data/GaitData/ckpt_dir', data_root='/data/FallDownData/URFD/video', dataset='URFD', device_ids='0', enable_guide=True, img_size=144, input_file='../../../preprocess/data/person_detection_and_tracking_results_drop-URFD.pkl', mean_dataset='kinetics', merge_type='', mode='test', model_arch='AGNet-pretrain', model_depth=18, multi_gpu=True, n_groups=-1, n_threads=8, no_mean_norm=False, norm_value=255, num_units=256, precrop=False, pretrained_path='', sample_duration=16, sample_size=112, std_norm=False, test_fold=1, train_crop='', with_segmentation=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load configurations\n",
    "conf_dict = yaml.load(open('../cfg/AGNet_URFD.yaml'), Loader=yaml.FullLoader)\n",
    "conf_dict = {k: '' if v is None else v for k,v in conf_dict.items()}\n",
    "opt = SimpleNamespace(**conf_dict)\n",
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07cc001732244cc96feff47e4695c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b8edcadff6404dbeb8a6bd28ceafdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load trained model from /data/GaitData/ckpt_dir/NonAttentive_AGNet-pretrain_r2plus1d_18-18_URFD/model_fold-1.pth...\n"
     ]
    }
   ],
   "source": [
    "opt, net, criterion1, criterion2, optimizer, scheduler, spatial_transform, temporal_transform, target_transform, plotter, train_loader, test_loader, target_columns = utils.data.prepare_data(\n",
    "    opt, opt.test_fold)\n",
    "net = load_trained_ckpt(opt, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from utils.validate_activations import ActivationMapProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init ActivationMapProvider\n",
    "actmap_provider = ActivationMapProvider(net, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalization(img, mean, std):\n",
    "    return img.permute(1, 2, 0).numpy()*std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "test_ds = test_loader.dataset\n",
    "layer_sel = f\"{opt.model_arch.split('-')[0].lower()}.conv_1x1\"\n",
    "\n",
    "import tqdm\n",
    "\n",
    "i = 1500  # falling-sample index    \n",
    "img_tensor, mask_tensor, target, vid, _ = test_ds[i]\n",
    "print(target)\n",
    "if torch.cuda.is_available():\n",
    "    img_tensor = img_tensor.cuda()\n",
    "\n",
    "activation_result = actmap_provider.compute(\n",
    "    img_tensor=img_tensor, activation_layer=layer_sel)\n",
    "\n",
    "# Move CUDA tensor to CPU\n",
    "img_tensor = img_tensor.cpu()\n",
    "\n",
    "overlay_seq = []\n",
    "\n",
    "for t in range(img_tensor.size(1)):\n",
    "    img_ = np.uint8(\n",
    "        255*denormalization(img_tensor[:, t], opt.mean, opt.std))\n",
    "    heatmap_ = np.uint8(255*activation_result[t])\n",
    "    heatmap_ = cv2.applyColorMap(heatmap_, cv2.COLORMAP_JET)\n",
    "    frame = np.uint8(heatmap_*0.3 + img_*0.5)\n",
    "    overlay_seq.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_filename = f'falling-sample-{opt.model_indicator}.avi'\n",
    "out = cv2.VideoWriter(video_filename,\n",
    "                      cv2.VideoWriter_fourcc(*'DIVX'),\n",
    "                      24.0, (opt.sample_size, opt.sample_size))\n",
    "for frame in overlay_seq:\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()\n",
    "\n",
    "video_name = os.path.splitext(video_filename)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -i \"{video_name}.avi\" \"{video_name}.mp4\" -y -loglevel quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"falling-sample-NonAttentive_AGNet-pretrain_r2plus1d_18-18.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_code = f\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"{video_name}.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(html_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "detection_data = pd.read_pickle('../../../preprocess/data/person_detection_and_tracking_results_drop-URFD.pkl')\n",
    "detection_data[detection_data.vids==vid].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = pd.read_csv('/data/FallDownData/URFD/TrainTestlist/trainlist01.txt',sep=' ', names=['vids', 'cls']).vids\n",
    "import collections\n",
    "d = collections.defaultdict(list)\n",
    "for v in train_list:\n",
    "    d[v[-8:]].append(v)\n",
    "{ k : len(d[k]) for k in d.keys() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = pd.read_csv('/data/FallDownData/URFD/TrainTestlist/trainlist01.txt',sep=' ', names=['vids', 'cls']).vids\n",
    "import collections\n",
    "d = collections.defaultdict(list)\n",
    "for v in train_list:\n",
    "    d[v.split('/')[0]].append(v)\n",
    "{ k : len(d[k]) for k in d.keys() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = pd.read_csv('/data/FallDownData/URFD/TrainTestlist/testlist01.txt',sep=' ', names=['vids', 'cls']).vids\n",
    "import collections\n",
    "d = collections.defaultdict(list)\n",
    "for v in train_list:\n",
    "    d[v[-8:]].append(v)\n",
    "{ k : len(d[k]) for k in d.keys() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = pd.read_csv('/data/FallDownData/URFD/TrainTestlist/testlist01.txt',sep=' ', names=['vids', 'cls']).vids\n",
    "import collections\n",
    "d = collections.defaultdict(list)\n",
    "for v in train_list:\n",
    "    d[v.split('/')[0]].append(v)\n",
    "{ k : len(d[k]) for k in d.keys() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/FallDownData/URFD/TrainTestlist/trainlist01.txt') as f:\n",
    "    lines = [ line.strip() for line in f.readlines() ]\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 2 : performance metrices (classification_report, AUROC, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grp_names['fold'].append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import collections\n",
    "f = h5py.File('../results.hdf5')\n",
    "full_grp_names = collections.defaultdict(list)\n",
    "\n",
    "def group_cb(name):\n",
    "    if name.endswith('_vals'):\n",
    "        name_splits = name.split('/')\n",
    "        fold = name_splits[1]\n",
    "        full_grp_names[fold].append('/'.join(name_splits[2:]))\n",
    "\n",
    "f.visit(group_cb)\n",
    "full_grp_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['MulticamFD_NonAttentive_AGNet-pretrain_r2plus1d_18-18']['fold-9']['test']['epoch-1']['true_vals'].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 3 : YOLO 의 오검출 케이스에 대한 AGNet 의 활성화맵 확인\n",
    "- YOLO(C++) converting to pytorch model\n",
    "- how to find false detection?\n",
    "    1) false alarm : FP\n",
    "    2) missing : FN\n",
    "- missing case에서의 YOLO 의 사람에 대한 activation과 AGNet의 activation 비교?\n",
    "    => YOLO가 잘 찾지 못하는 부분을 AGNet은 따라갈 수 있더라...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
