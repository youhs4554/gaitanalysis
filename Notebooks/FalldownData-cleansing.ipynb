{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import natsort\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import zipfile\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data 1 : URFD\n",
    "- step 1: unzip files and save frames\n",
    "- step 2: read CSV where frame-level labels are given\n",
    "- step 3: segment frames based on frame-level labels\n",
    "- step 4: pack each directory containing segmented .jpg files as .avi file(to follow UCF data pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adl videos ==============\n",
      "fall videos ==============\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91dda38de17d4e34a8a2ad020bd7e410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=130.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# step 1 : unzip files and save frames\n",
    "\n",
    "raw_folder = \"/data/FallDownData/URFD_new/raw/\"\n",
    "data_folder = \"/data/FallDownData/URFD_new/frames_not_segmented/\"\n",
    "\n",
    "adl_folder = \"adl/\"\n",
    "fall_folder = \"fall/\"\n",
    "\n",
    "# Path to save the frames\n",
    "output_path = \"/data/FallDownData/URFD_new/frames/\"\n",
    "\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder + fall_folder)\n",
    "    os.makedirs(data_folder + adl_folder)\n",
    "\n",
    "adl_zipped_files = glob.glob(raw_folder + 'adl-*-cam0-rgb.zip')\n",
    "fall_zipped_files = glob.glob(raw_folder + 'fall-*-cam0-rgb.zip')\n",
    "\n",
    "content = [\n",
    "    [adl_zipped_files, data_folder + adl_folder],\n",
    "    [fall_zipped_files, data_folder + fall_folder]\n",
    "]\n",
    "for zipped_files, dst_folder in content:\n",
    "    for zipped_file in zipped_files:\n",
    "        zfile = zipfile.ZipFile(zipped_file)\n",
    "        zfile.extractall(dst_folder)\n",
    "\n",
    "\n",
    "# step 2 : read CSV where frame-level labels are given\n",
    "\n",
    "falls_labels = \"/data/FallDownData/URFD/urfall-cam0-falls.csv\"\n",
    "notfalls_labels = \"/data/FallDownData/URFD/urfall-cam0-adls.csv\"\n",
    "\n",
    "labels = {'falls': dict(), 'notfalls': dict()}\n",
    "\n",
    "# For falls videos: read the CSV where frame-level labels are given\n",
    "with open(falls_labels, 'r') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    event_type = 'falls'\n",
    "    for row in spamreader:\n",
    "        elems = row[0].split(',')  # read a line in the csv\n",
    "        if not elems[0] in labels[event_type]:\n",
    "            labels[event_type][elems[0]] = []\n",
    "        if int(elems[2]) == 1 or int(elems[2]) == -1:\n",
    "            labels[event_type][elems[0]].append(0)\n",
    "        elif int(elems[2]) == 0:\n",
    "            labels[event_type][elems[0]].append(1)\n",
    "\n",
    "# For ADL videos: read the CSV where frame-level labels are given\n",
    "with open(notfalls_labels, 'r') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    event_type = 'notfalls'\n",
    "    for row in spamreader:\n",
    "        elems = row[0].split(',')  # read a line in the csv\n",
    "        if not elems[0] in labels[event_type]:\n",
    "            labels[event_type][elems[0]] = []\n",
    "        if int(elems[2]) == 1 or int(elems[2]) == -1:\n",
    "            labels[event_type][elems[0]].append(0)\n",
    "        elif int(elems[2]) == 0:\n",
    "            labels[event_type][elems[0]].append(1)\n",
    "\n",
    "# step 3 : segment frames based on frame-level labels\n",
    "\n",
    "# Get all folders: each one contains the set of images of the video\n",
    "folders = [f for f in os.listdir(data_folder)\n",
    "           if os.path.isdir(os.path.join(data_folder, f))]\n",
    "\n",
    "for folder in folders:\n",
    "    print('{} videos =============='.format(folder))\n",
    "    events = [f for f in os.listdir(data_folder + folder)\n",
    "              if os.path.isdir(os.path.join(data_folder + folder, f))]\n",
    "    events.sort()\n",
    "    for nb_event, event, in enumerate(events):\n",
    "        # Create the appropriate folder\n",
    "        if folder == 'adl':\n",
    "            event_id = event[:6]\n",
    "            new_folder = output_path + 'adl/{}'.format(event)\n",
    "            if not os.path.exists(new_folder):\n",
    "                os.makedirs(new_folder)\n",
    "        elif folder == 'fall':\n",
    "            event_id = event[:7]\n",
    "            new_folder = output_path + 'fall/{}'.format(event)\n",
    "            if not os.path.exists(new_folder):\n",
    "                os.makedirs(new_folder)\n",
    "\n",
    "        path_to_images = data_folder + folder + '/' + event + '/'\n",
    "\n",
    "        # Load all the images of the video\n",
    "        images = [f for f in os.listdir(path_to_images)\n",
    "                  if os.path.isfile(os.path.join(path_to_images, f))]\n",
    "        images.sort()\n",
    "        fall_detected = False  # whether a fall has been detected in the video\n",
    "        frame_counts = {\n",
    "            \"post\": 0,\n",
    "            \"pre\": 0,\n",
    "            \"fall\": 0\n",
    "        }\n",
    "        for nb_image, image in enumerate(images):\n",
    "            x = cv2.imread(path_to_images + image)\n",
    "\n",
    "            # If the image is part of an ADL video no fall need to be\n",
    "            # considered\n",
    "            if folder == 'adl':\n",
    "                # Save the image\n",
    "                save_path = (output_path +\n",
    "                             'adl/{}'.format(event) +\n",
    "                             '/thumb{:05}.jpg'.format(nb_image+1))\n",
    "                cv2.imwrite(save_path, x)\n",
    "            elif folder == 'fall':\n",
    "                event_type = 'falls'\n",
    "                if labels[event_type][event_id][nb_image] == 0:  # ADL\n",
    "                    if fall_detected:\n",
    "                        # Create another folder for an ADL event,\n",
    "                        # i.e. the post-fall ADL event\n",
    "                        new_folder = (output_path +\n",
    "                                      'adl/{}_post'.format(event))\n",
    "                        if not os.path.exists(new_folder):\n",
    "                            os.makedirs(new_folder)\n",
    "                        \n",
    "                        frame_counts[\"post\"] += 1\n",
    "\n",
    "                        save_path = (output_path +\n",
    "                                     'adl/{}_post'.format(event) +\n",
    "                                     '/thumb{:05}.jpg'.format(frame_counts[\"post\"]))\n",
    "                    else:\n",
    "                        new_folder = (output_path +\n",
    "                                      'adl/{}_pre'.format(event))\n",
    "                        if not os.path.exists(new_folder):\n",
    "                            os.makedirs(new_folder)\n",
    "                        \n",
    "                        frame_counts[\"pre\"] += 1\n",
    "                        \n",
    "                        save_path = (output_path +\n",
    "                                     'adl/{}_pre'.format(event) +\n",
    "                                     '/thumb{:05}.jpg'.format(frame_counts[\"pre\"]))\n",
    "                    cv2.imwrite(save_path, x)\n",
    "\n",
    "                elif labels[event_type][event_id][nb_image] == 1:  # actual fall\n",
    "                    new_folder = (output_path +\n",
    "                                      'fall/{}'.format(event))\n",
    "                    if not os.path.exists(new_folder):\n",
    "                        os.makedirs(new_folder)\n",
    "\n",
    "                    frame_counts[\"fall\"] += 1\n",
    "\n",
    "                    save_path = (output_path +\n",
    "                                 'fall/{}'.format(event) +\n",
    "                                 '/thumb{:05}.jpg'.format(frame_counts[\"fall\"]))\n",
    "                    cv2.imwrite(save_path, x)\n",
    "                    # If fall is detected in a video set the variable to True\n",
    "                    # used to discern between pre- and post-fall ADL events\n",
    "                    fall_detected = True\n",
    "\n",
    "\n",
    "# step 4 : pack each directory containing segmented .jpg files as .avi file (to follow UCF data pipeline)\n",
    "\n",
    "root = '/data/FallDownData/URFD_new/frames'\n",
    "video_files = natsort.natsorted(glob.glob(root + '/*/*'))\n",
    "for video in tqdm(video_files):\n",
    "    prefix, ext = os.path.splitext(video)\n",
    "    save_path = prefix.replace('frames', 'video') + '.avi'\n",
    "\n",
    "    frame_list = natsort.natsorted(glob.glob(video+'/*.jpg'))\n",
    "    first_frame = cv2.imread(frame_list[0])\n",
    "    out = cv2.VideoWriter(save_path,\n",
    "                          cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'),\n",
    "                          30, (first_frame.shape[:-1][::-1]))\n",
    "    for img_file in frame_list:\n",
    "        img_arr = cv2.imread(img_file)\n",
    "        out.write(img_arr)\n",
    "\n",
    "    os.system('mkdir -p {}'.format(os.path.dirname(save_path)))\n",
    "    out.release()\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data 2 : Multicam FDD\n",
    "- this dataset is bullshit!\n",
    "- syncronization is not matched well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163706d8ef2c45a7a84777582d5cb107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='packing frames as a video...'), FloatProgress(value=0.0, max=1304.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "fall_annotation_file = \"/data/FallDownData/MulticamFD/Multicam_Annotations.csv\"\n",
    "delays_file = \"/data/FallDownData/MulticamFD_new/delays_multicam.json\"\n",
    "data_folder = \"/data/FallDownData/MulticamFD_new/raw/\"\n",
    "output_folder = \"/data/FallDownData/MulticamFD_new/frames/\"\n",
    "\n",
    "num_cameras = 8\n",
    "num_scenraios = 24\n",
    "\n",
    "# read annotation files(falling time intervals & camera delay for sync)\n",
    "with open(fall_annotation_file, \"r\") as f:\n",
    "    annotations = pd.read_csv(fall_annotation_file)\n",
    "with open(delays_file, \"r\") as f:\n",
    "    delays = json.load(f)\n",
    "        \n",
    "for s in trange(1, num_scenraios+1, desc=\"scenario loop\"):\n",
    "    # get all videos\n",
    "    videos = glob.glob(data_folder+\"chute{:02d}/*\".format(s))\n",
    "    videos.sort()\n",
    "    \n",
    "    cur_anno = annotations[annotations.id==s].values\n",
    "        \n",
    "    for cam, video in tqdm(list(enumerate(videos, 1)), desc=\"camera loop\", leave=False):\n",
    "        cap = cv2.VideoCapture(video)\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        # Delay of this camara for this scenario\n",
    "        delay = delays[\"camera{}\".format(cam)][str(s)]\n",
    "        delay_pos = 0\n",
    "        for seq_pos in range(len(cur_anno)):\n",
    "            *_, start, end, code = cur_anno[seq_pos]\n",
    "            \n",
    "            # apply delay\n",
    "            start += delay if not isinstance(delay, list) else delay[min(delay_pos, len(delay)-1)]\n",
    "            end += delay if not isinstance(delay, list) else delay[min(delay_pos, len(delay)-1)]\n",
    "            \n",
    "            # cursor which starts from delayed \"start\"\n",
    "            pos = start\n",
    "            \n",
    "            # move cursor to start\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "            \n",
    "            # gt-label\n",
    "            label = 'fall' if code == 2 else 'adl'\n",
    "            \n",
    "            if label == 'fall':\n",
    "                # increase delay_pos\n",
    "                delay_pos += 1\n",
    "                \n",
    "            while pos < end:\n",
    "                ret, frame = cap.read()\n",
    "                pos += 1\n",
    "                output_path = (\n",
    "                    output_folder +\n",
    "                    '{}/chute{:02}-cam{}-s{:02}/'.format(\n",
    "                    label, s, cam, seq_pos+1\n",
    "                ))\n",
    "                if not os.path.exists(output_path):\n",
    "                    os.makedirs(output_path, exist_ok=True)\n",
    "                cv2.imwrite(output_path + 'thumb{:05d}.jpg'.format(pos-start), frame)\n",
    "                \n",
    "\n",
    "# pack each directory containing segmented .jpg files as .avi file (to follow UCF data pipeline)\n",
    "root = '/data/FallDownData/MulticamFD_new/frames'\n",
    "video_files = natsort.natsorted(glob.glob(root + '/*/*'))\n",
    "for video in tqdm(video_files, desc=\"packing frames as a video...\"):\n",
    "    prefix, ext = os.path.splitext(video)\n",
    "    save_path = prefix.replace('frames', 'video') + '.avi'\n",
    "\n",
    "    frame_list = natsort.natsorted(glob.glob(video+'/*.jpg'))\n",
    "    first_frame = cv2.imread(frame_list[0])\n",
    "    out = cv2.VideoWriter(save_path,\n",
    "                          cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'),\n",
    "                          120, (first_frame.shape[:-1][::-1]))\n",
    "    for img_file in frame_list:\n",
    "        img_arr = cv2.imread(img_file)\n",
    "        out.write(img_arr)\n",
    "\n",
    "    os.system('mkdir -p {}'.format(os.path.dirname(save_path)))\n",
    "    out.release()\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Statistics of detailed action sequences!\n",
      "----------------------------------------\n",
      "Counter({'Walking, standing up': 36,\n",
      "         'Moving up': 29,\n",
      "         'Moving down': 26,\n",
      "         'Falling': 25,\n",
      "         'Lying on the ground': 23,\n",
      "         'Crounching': 11,\n",
      "         'Sitting': 9,\n",
      "         'Lying on a sofa': 4})\n",
      "\n",
      "### number of normal sequences:  138 // number of falling sequences:  25\n"
     ]
    }
   ],
   "source": [
    "ix2label = {1: \"Walking, standing up\",\n",
    "2 : \"Falling\",\n",
    "3 : \"Lying on the ground\",\n",
    "4 : \"Crounching\",\n",
    "5 : \"Moving down\",\n",
    "6 : \"Moving up\",\n",
    "7 : \"Sitting\",\n",
    "8 : \"Lying on a sofa\",\n",
    "9 : \"Moving horizontaly\"}\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "foo = Counter(annotations.code.map(ix2label))\n",
    "\n",
    "print(\"-\"*40)\n",
    "print(\"Statistics of detailed action sequences!\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "pprint(foo)\n",
    "\n",
    "n_falls = foo.pop(\"Falling\")\n",
    "n_normals = sum(foo.values())\n",
    "\n",
    "print()\n",
    "print(\"### number of normal sequences: \", n_normals, \"// number of falling sequences: \", n_falls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Traintests-plit for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, LeaveOneOut\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_traintestList(root, annotation_path, random_state=0, n_splits=5):\n",
    "    dataset_name = os.path.basename(os.path.dirname(root.rstrip('/')))\n",
    "    \n",
    "    os.system(f'mkdir -p {annotation_path}')\n",
    "\n",
    "    video_dirs = natsort.natsorted(glob.glob(root + '/*/*'))\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(video_dirs)\n",
    "\n",
    "    class2idx = {'adl':0, 'fall':1}\n",
    "\n",
    "    formated_video_dirs = np.array([ x[len(root.rstrip('/'))+1:] for x in video_dirs ]).tolist()\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "    training_data = []\n",
    "    leaveOut_data = []\n",
    "    \n",
    "    if dataset_name == 'URFD_new':\n",
    "        for j in range(len(formated_video_dirs)):\n",
    "            vid = formated_video_dirs[j]\n",
    "            if \"cam0\" in vid:\n",
    "                training_data.append(vid)\n",
    "    elif dataset_name == 'MulticamFD_new':\n",
    "        for j in range(len(formated_video_dirs)):\n",
    "            vid = formated_video_dirs[j]\n",
    "            if \"chute23\" in vid or \"chute24\" in vid:\n",
    "                # do not consider mixed sequences\n",
    "                continue\n",
    "            training_data.append(vid)\n",
    "            \n",
    "            #if \"chute23\" in vid or \"chute24\" in vid:\n",
    "            #    leaveOut_data.append(vid) # confounding samples are excluded for training data\n",
    "            #else:\n",
    "            #    training_data.append(vid)\n",
    "    \n",
    "    for k, (train_ix, test_ix) in enumerate(kf.split(training_data, [os.path.dirname(x) for x in training_data])):\n",
    "        print()\n",
    "        _train, _test = np.array(training_data)[train_ix], np.array(training_data)[test_ix]\n",
    "        \n",
    "        # include leaveOut_data into test data\n",
    "        if len(leaveOut_data) > 0:\n",
    "            _test = np.append(_test, leaveOut_data)\n",
    "        \n",
    "        _train_lab = [ os.path.dirname(x) for x in _train ]\n",
    "        _test_lab = [ os.path.dirname(x) for x in _test ]\n",
    "        print()\n",
    "        print(f'[splist-{k}] train : {np.unique(_train_lab, return_counts=True)}, test : {np.unique(_test_lab, return_counts=True)}')\n",
    "        for _split, _data in zip(['train', 'test'], [_train, _test]):\n",
    "            lines = []\n",
    "            for i in range(len(_data)):\n",
    "                line = os.path.splitext(_data[i])[0] + \" \"\n",
    "                line += str(class2idx[os.path.dirname(_data[i])])\n",
    "\n",
    "                lines.append(line + '\\n')\n",
    "\n",
    "            with open(os.path.join(annotation_path, f\"{_split}list{k+1:02d}.txt\"), 'w') as fp:\n",
    "                fp.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[splist-0] train : (array(['adl', 'fall'], dtype='<U4'), array([80, 24])), test : (array(['adl', 'fall'], dtype='<U4'), array([20,  6]))\n",
      "\n",
      "\n",
      "[splist-1] train : (array(['adl', 'fall'], dtype='<U4'), array([80, 24])), test : (array(['adl', 'fall'], dtype='<U4'), array([20,  6]))\n",
      "\n",
      "\n",
      "[splist-2] train : (array(['adl', 'fall'], dtype='<U4'), array([80, 24])), test : (array(['adl', 'fall'], dtype='<U4'), array([20,  6]))\n",
      "\n",
      "\n",
      "[splist-3] train : (array(['adl', 'fall'], dtype='<U4'), array([80, 24])), test : (array(['adl', 'fall'], dtype='<U4'), array([20,  6]))\n",
      "\n",
      "\n",
      "[splist-4] train : (array(['adl', 'fall'], dtype='<U4'), array([80, 24])), test : (array(['adl', 'fall'], dtype='<U4'), array([20,  6]))\n"
     ]
    }
   ],
   "source": [
    "# URFD\n",
    "create_traintestList(root = '/data/FallDownData/URFD_new/video',\n",
    "                     annotation_path = '/data/FallDownData/URFD_new/TrainTestlist',\n",
    "                     n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[splist-0] train : (array(['adl', 'fall'], dtype='<U4'), array([582, 147])), test : (array(['adl', 'fall'], dtype='<U4'), array([146,  37]))\n",
      "\n",
      "\n",
      "[splist-1] train : (array(['adl', 'fall'], dtype='<U4'), array([582, 147])), test : (array(['adl', 'fall'], dtype='<U4'), array([146,  37]))\n",
      "\n",
      "\n",
      "[splist-2] train : (array(['adl', 'fall'], dtype='<U4'), array([582, 148])), test : (array(['adl', 'fall'], dtype='<U4'), array([146,  36]))\n",
      "\n",
      "\n",
      "[splist-3] train : (array(['adl', 'fall'], dtype='<U4'), array([583, 147])), test : (array(['adl', 'fall'], dtype='<U4'), array([145,  37]))\n",
      "\n",
      "\n",
      "[splist-4] train : (array(['adl', 'fall'], dtype='<U4'), array([583, 147])), test : (array(['adl', 'fall'], dtype='<U4'), array([145,  37]))\n"
     ]
    }
   ],
   "source": [
    "# MulticamFD\n",
    "create_traintestList(root = '/data/FallDownData/MulticamFD_new/video',\n",
    "                     annotation_path = '/data/FallDownData/MulticamFD_new/TrainTestlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
